[["index.html", "Final Project Introduction View GitHub Code 0.1 Methods 0.2 Site Description 0.3 Data Acquisition and Plotting Tests 0.4 Interactive Graph 0.5 Information about the Poudre River", " Final Project Madeline Bean 2022-03-28 Introduction This bookdown website is comprised of all assignments completed in Introduction to Data Science Course ESS 580A7. I completed this class in my spring semester of 2022 while getting my Professional Science Master’s in Ecosystem Sciences and Sustainability. View GitHub Code The code used to build this R Bookdown is hosted on GitHub. Final Project GitHub #Poudre River Interactive Graph The Poudre River runs through northern Colorado, passing through Fort Collins, CO. In this assignment, we look at the the annual discharge of the river to determine annual patterns and severe weather events. 0.1 Methods The Poudre River at Lincoln Bridge is: Downstream of only a little bit of urban stormwater Near Odell Brewing CO Near an open space area and the Poudre River Trail Downstream of many agricultural diversions 0.2 Site Description 0.3 Data Acquisition and Plotting Tests 0.3.1 Data Download q &lt;- readNWISdv(siteNumbers = &#39;06752260&#39;, parameterCd = &#39;00060&#39;, startDate = &#39;2017-01-01&#39;, endDate = &#39;2022-01-01&#39;) %&gt;% rename(q = &#39;X_00060_00003&#39;) 0.3.2 Static Data Plotter ggplot(q, aes(x = Date, y = q)) + geom_line() + ylab(&#39;Q (cfs)&#39;) + ggtitle(&#39;Discharge in the Poudre River, Fort Collins&#39;) 0.3.3 Interactive Data Plotter q_xts &lt;- xts(q$q, order.by = q$Date) #plotting dygraph(q_xts) %&gt;% dyAxis(&quot;y&quot;, label = &quot;Discharge (cfs)&quot;) 0.4 Interactive Graph #creating an interactive graph dygraph(q_xts) %&gt;% dyOptions(drawPoints = TRUE, pointSize = 2) 0.5 Information about the Poudre River The History of the Poudre River The Cache la Poudre River got its name from a group of French trappers that hid their gun powder on the banks of the river to lighten their wagons. Hence the translation of the name to be the “hiding place of the powder.” Since the 1800’s, the Poudre River has been a vital resource for the Northern Colorado community. From industrial to agricultural and to residential use, the water from the Poudre river is in great demand. Poudre River Geomorphology The Poudre River starts high in the Rocky Mountain National Park peaks and flows north east through the Roosevelt National Forest, slowly making its way through the city of Fort Collins. There are a lot of recreational activities that occur along the river, including hiking, biking, kayaking, and white water rafting. However, because of the multiple reservoirs that consume water from the Poudre, the once rapid, flowing river is now only a trickling stream in some areas, especially during the winter months. "],["hayman-fire-recovery.html", "Chapter 1 Hayman Fire Recovery 1.1 Reading in the Data 1.2 NDVI and NDMI Correlation 1.3 Average NDSI and NDVI Correlation 1.4 Snow Effect Differences 1.5 Average Greenest Month 1.6 Average Snowiest Month", " Chapter 1 Hayman Fire Recovery The Hayman Fire occurred on June 8th, 2002. This was the largest wildfire in Colorado’s history until 2020, burning over 138,000 acres of land. In this assignment, we looked at how the fire affected vegetation growth in the area. library(tidyverse) library(tidyr) library(ggthemes) library(lubridate) library(ggpubr) # Now that we have learned how to munge (manipulate) data # and plot it, we will work on using these skills in new ways knitr::opts_knit$set(root.dir=&#39;..&#39;) 1.1 Reading in the Data ####-----Reading in Data and Stacking it ----- #### #Reading in files files &lt;- list.files(&#39;02-hw-hayman&#39;,full.names=T) #Read in individual data files ndmi &lt;- read_csv(&#39;/Users/maddiebean21/Desktop/School/ESS580A7/bookdown_final/data/02-hw-hayman/hayman_ndmi.csv&#39;) %&gt;% rename(burned=2,unburned=3) %&gt;% mutate(data=&#39;ndmi&#39;) ndsi &lt;- read_csv(&#39;/Users/maddiebean21/Desktop/School/ESS580A7/bookdown_final/data/02-hw-hayman/hayman_ndsi.csv&#39;) %&gt;% rename(burned=2,unburned=3) %&gt;% mutate(data=&#39;ndsi&#39;) ndvi &lt;- read_csv(&#39;/Users/maddiebean21/Desktop/School/ESS580A7/bookdown_final/data/02-hw-hayman/hayman_ndvi.csv&#39;)%&gt;% rename(burned=2,unburned=3) %&gt;% mutate(data=&#39;ndvi&#39;) # Stack as a tidy dataset full_long &lt;- rbind(ndvi,ndmi,ndsi) %&gt;% gather(key=&#39;site&#39;,value=&#39;value&#39;,-DateTime,-data) %&gt;% filter(!is.na(value)) 1.2 NDVI and NDMI Correlation In order to find the correlation between NDVI and NDMI, we converted the full_long dataset into a wide dataset using the function “spread” and then made a plot of this function, grouped by unburned vs burned sites. We excluded winter months and focused on summer months because vegetation does not generally grow in the winter. #changing data into wide full_wide &lt;- spread(data=full_long,key=&#39;data&#39;,value=&#39;value&#39;) %&gt;% filter_if(is.numeric,all_vars(!is.na(.))) %&gt;% mutate(month = month(DateTime), year = year(DateTime)) #filtering summer months summer_only &lt;- filter(full_wide,month %in% c(6,7,8,9)) #plotting summer months of variables ggplot(summer_only,aes(x=ndmi,y=ndvi,color=site)) + geom_point() + theme_few() + scale_color_few() + theme(legend.position=c(0.8,0.8)) There is a strong, positive correlation between NDMI and NDVI. 1.3 Average NDSI and NDVI Correlation In order to find the correlation between the average NDSI and NDVI, we used the data points for NDSI only during January - April (snow season) and only data points for NDVI during June - August (vegetation growth season). We found that there is a low positive correlation of 0.180 between snow coverage and vegetation. This means that the previous year’s snow cover has little effect on the vegetation growth for the following summer and that this correlation could be from other factors in the environment. Looking at the graph, you can see a difference in correlation between NDVI and NDSI burned and un-burned sites. #variable ndsi months var.snow_months &lt;- c(1,2,3,4) #variable ndvi months var.growth_months &lt;- c(6,7,8) #mean NDSI per year ndsi_avg &lt;- full_wide[c(&quot;DateTime&quot;, &quot;ndsi&quot;, &quot;month&quot;, &quot;year&quot;, &quot;site&quot;)] %&gt;% filter(month %in% var.snow_months) %&gt;% group_by(site, year) %&gt;% summarize(ndsi_avg=mean(ndsi)) #mean NDVI per year ndvi_avg &lt;- full_wide[c(&quot;DateTime&quot;, &quot;ndvi&quot;, &quot;month&quot;, &quot;year&quot;, &quot;site&quot;)] %&gt;% filter(month %in% var.growth_months) %&gt;% group_by(site, year) %&gt;% summarize(ndvi_avg=mean(ndvi)) #combining NDVI and NDSI into one dataset combined &lt;- inner_join(ndvi_avg, ndsi_avg) #correlation cor(combined$ndvi_avg, combined$ndsi_avg) ## [1] 0.1803564 ggplot(combined, aes(x=ndsi_avg, y=ndvi_avg, color=site)) + geom_point() + theme_few() + scale_color_few() + theme(legend.position=c(0.8,0.8)) 1.4 Snow Effect Differences We then looked at the difference of snow effect from the previous correlation between pre- and post-burn and burned and unburned. #snow effect pre-burn preburn &lt;- c(1984:2001) #snow effect post-burn postburn &lt;- c(2003:2019) #snow effect average for preburn ndsi_preburn_avg &lt;- full_wide[c(&quot;DateTime&quot;, &quot;ndsi&quot;, &quot;month&quot;, &quot;year&quot;, &quot;site&quot;)] %&gt;% filter(month %in% var.snow_months) %&gt;% filter(year %in% preburn) %&gt;% group_by(site, year) %&gt;% summarize(ndsi_preburn_avg=mean(ndsi)) #vegetation effect average for preburn ndvi_preburn_avg &lt;- full_wide[c(&quot;DateTime&quot;, &quot;ndvi&quot;, &quot;month&quot;, &quot;year&quot;, &quot;site&quot;)] %&gt;% filter(month %in% var.growth_months) %&gt;% filter(year %in% preburn) %&gt;% group_by(site, year) %&gt;% summarize(ndvi_preburn_avg=mean(ndvi)) #combing preburn ndsi and ndvi into one data set combine_preburn &lt;- inner_join(ndsi_preburn_avg, ndvi_preburn_avg) #correlation for preburn cor(combine_preburn$ndsi_preburn_avg, combine_preburn$ndvi_preburn_avg) ## [1] 0.09100615 #snow effect average for postburn ndsi_postburn_avg &lt;- full_wide[c(&quot;DateTime&quot;, &quot;ndsi&quot;, &quot;month&quot;, &quot;year&quot;, &quot;site&quot;)] %&gt;% filter(month %in% var.snow_months) %&gt;% filter(year %in% postburn) %&gt;% group_by(site, year) %&gt;% summarize(ndsi_postburn_avg=mean(ndsi)) #vegetation effect average for postburn ndvi_postburn_avg &lt;- full_wide[c(&quot;DateTime&quot;, &quot;ndvi&quot;, &quot;month&quot;, &quot;year&quot;, &quot;site&quot;)] %&gt;% filter(month %in% var.growth_months) %&gt;% filter(year %in% postburn) %&gt;% group_by(site, year) %&gt;% summarize(ndvi_postburn_avg=mean(ndvi)) #combing postburn ndsi and ndvi into one data set combine_postburn &lt;- inner_join(ndsi_postburn_avg, ndvi_postburn_avg) #correlation for postburn cor(combine_postburn$ndsi_postburn_avg, combine_postburn$ndvi_postburn_avg) ## [1] 0.24394 #snow effect average for burned ndsi_burned_avg &lt;- full_wide[c(&quot;DateTime&quot;, &quot;ndsi&quot;, &quot;month&quot;, &quot;year&quot;, &quot;site&quot;)] %&gt;% filter(month %in% var.snow_months) %&gt;% filter(site %in% &quot;burned&quot;) %&gt;% group_by(site, year) %&gt;% summarize(ndsi_burned_avg=mean(ndsi)) #snow effect average for unburned ndsi_unburned_avg &lt;- full_wide[c(&quot;DateTime&quot;, &quot;ndsi&quot;, &quot;month&quot;, &quot;year&quot;, &quot;site&quot;)] %&gt;% filter(month %in% var.snow_months) %&gt;% filter(site %in% &quot;unburned&quot;) %&gt;% group_by(site, year) %&gt;% summarize(ndsi_unburned_avg=mean(ndsi)) #vegetation effect average for burned ndvi_burned_avg &lt;- full_wide[c(&quot;DateTime&quot;, &quot;ndvi&quot;, &quot;month&quot;, &quot;year&quot;, &quot;site&quot;)] %&gt;% filter(month %in% var.growth_months) %&gt;% filter(site %in% &quot;burned&quot;) %&gt;% group_by(site, year) %&gt;% summarize(ndvi_burned_avg=mean(ndvi)) #vegetation effect average for unburned ndvi_unburned_avg &lt;- full_wide[c(&quot;DateTime&quot;, &quot;ndvi&quot;, &quot;month&quot;, &quot;year&quot;, &quot;site&quot;)] %&gt;% filter(month %in% var.growth_months) %&gt;% filter(site %in% &quot;unburned&quot;) %&gt;% group_by(site, year) %&gt;% summarize(ndvi_unburned_avg=mean(ndvi)) #combining data for burned combined_burned &lt;- inner_join(ndvi_burned_avg, ndsi_burned_avg) #combining data for unburned combined_unburned &lt;- inner_join(ndvi_unburned_avg, ndsi_unburned_avg) #correlation for burned data cor(combined_burned$ndvi_burned_avg, combined_burned$ndsi_burned_avg) ## [1] 0.08700527 #correlation for unburned data cor(combined_unburned$ndvi_unburned_avg, combined_unburned$ndsi_unburned_avg) ## [1] -0.03100231 #graphing the data Preburngraph &lt;- ggplot(combine_preburn, aes(x=ndsi_preburn_avg, y=ndvi_preburn_avg)) + geom_point() + theme_few() + scale_color_few() + labs(x=&quot;Pre-burn NDSI Average&quot;, y=&quot;Pre-burn NDVI Average&quot;) Postburngraph &lt;- ggplot(combine_postburn, aes(x=ndsi_postburn_avg, y=ndvi_postburn_avg)) + geom_point() + theme_few() + scale_color_few()+ labs(x=&quot;Post-burn NDSI Average&quot;, y = &quot;Post-burn NDVI Average&quot;) Burnedgraph &lt;- ggplot(combined_burned, aes(x=ndsi_burned_avg, y=ndvi_burned_avg)) + geom_point() + theme_few() + scale_color_few()+ labs(x=&quot;NDSI Burned Average&quot;, y=&quot;NDVI Burned Average&quot;) Unburnedgraph &lt;- ggplot(combined_unburned, aes(x=ndsi_unburned_avg, y=ndvi_unburned_avg))+ geom_point() + theme_few() + scale_color_few()+ labs(x= &quot;Average NDSI Unburned&quot;, y=&quot;Average NDVI Unburned&quot;) #Plotting the data in one frame Plot &lt;- ggarrange(Preburngraph, Postburngraph, Burnedgraph, Unburnedgraph, labels = c(&quot;Pre-burned&quot;, &quot;Post-burned&quot;, &quot;Burned&quot;, &quot;Unburned&quot;), ncol = 2, nrow = 2) Plot For pre-burn snow effect, the correlation is 0.091. For post-burn snow effect, the correlation is 0.244. For un-burned snow effect, the correlation is -0.031 For burned snow effect, the correlation is 0.087. The snow effect from question 2 is a more generalized correlation. While all of these variables have a weak correlation, this question isolates the different variables in the data to help determine the relationship between snow coverage and vegetation growth. The post-burn condition has the strongest correlation between the previous year’s snow coverage and the vegetation growth for the following year. The weakest correlation between snow coverage and vegetation growth was for the un-burned condition. This is interesting because this could potentially mean that the Hayman Fire has pushed the vegetation growth to rely more on snow coverage to get their nutrients. This is still a weak correlation, so more research will need to be conducted to determine if this is true or not; there are many other variables in the environment that could have caused this correlation. 1.5 Average Greenest Month Here, we looked at which month was the greenest, on average. We discovered that August was the greenest month. #creating an object for the average vegetation for each month ndvigreenest &lt;- full_wide[c(&quot;DateTime&quot;, &quot;ndvi&quot;, &quot;month&quot;, &quot;year&quot;, &quot;site&quot;)] %&gt;% group_by(month) %&gt;% summarize(ndvi=mean(ndvi)) #Plotting the average vegetation for each month ggplot(ndvigreenest, aes(x=month, y=ndvi)) + geom_bar(stat=&quot;identity&quot;, fill=&quot;#009E73&quot;) + theme_minimal() 1.6 Average Snowiest Month Next, we looked at which month was the snowiest on average. January was the snowiest month. #creating snow object ndsisnowiest &lt;- full_wide[c(&quot;DateTime&quot;, &quot;ndsi&quot;, &quot;month&quot;, &quot;year&quot;, &quot;site&quot;)] %&gt;% group_by(month) %&gt;% summarize(ndsi=mean(ndsi)) #plotting the snow for each month ggplot(ndsisnowiest, aes(x=month, y=ndsi)) + geom_bar(stat=&quot;identity&quot;, fill=&quot;#56B4E9&quot;) + theme_minimal() "],["snow-data-example.html", "Chapter 2 Snow Data Example 2.1 Reading an html 2.2 Data Download 2.3 Data read-in 2.4 Extracting the meteorological data URLs 2.5 Download the meteorological data. 2.6 Custom function writing 2.7 Summary of meteorlogical files 2.8 Monthy average temperature plot 2.9 Average daily precipitation", " Chapter 2 Snow Data Example In this assignment, I explored web scraping, different functions and iterations by using a data set from the Center for Snow and Avalanche Studies Website and read a table in. This table contains links to data I want and to programatically download for three sites. I don’t know much about these sites, but they contain incredibly rich snow, temperature, and precip data. 2.1 Reading an html 2.1.1 Extract CSV links from webpage site_url &lt;- &#39;https://snowstudies.org/archived-data/&#39; #Read the web url webpage &lt;- read_html(site_url) #See if we can extract tables and get the data that way tables &lt;- webpage %&gt;% html_nodes(&#39;table&#39;) %&gt;% magrittr::extract2(3) %&gt;% html_table(fill = TRUE) #That didn&#39;t work, so let&#39;s try a different approach #Extract only weblinks and then the URLs! links &lt;- webpage %&gt;% html_nodes(&#39;a&#39;) %&gt;% .[grepl(&#39;24hr&#39;,.)] %&gt;% html_attr(&#39;href&#39;) 2.2 Data Download 2.2.1 Download data in a for loop #Grab only the name of the file by splitting out on forward slashes splits &lt;- str_split_fixed(links,&#39;/&#39;,8) #Keep only the 8th column dataset &lt;- splits[,8] #generate a file list for where the data goes file_names &lt;- paste0(&#39;/Users/maddiebean21/Desktop/School/ESS580A7/bookdown_final/data/03-hw-snow/SASP_24hr.csv&#39;,dataset) for(i in 1:3){ download.file(links[i],destfile=file_names[i]) } downloaded &lt;- file.exists(file_names) evaluate &lt;- !all(downloaded) 2.2.2 Download data in a map #Map version of the same for loop (downloading 3 files) if(evaluate == T){ map2(links[1:3],file_names[1:3],download.file) }else{print(&#39;data already downloaded&#39;)} ## [[1]] ## [1] 0 ## ## [[2]] ## [1] 0 ## ## [[3]] ## [1] 0 2.3 Data read-in 2.3.1 Read in just the snow data as a loop #Pattern matching to only keep certain files snow_files &lt;- file_names %&gt;% .[!grepl(&#39;SG_24&#39;,.)] %&gt;% .[!grepl(&#39;PTSP&#39;,.)] empty_data &lt;- list() snow_data &lt;- for(i in 1:length(snow_files)){ empty_data[[i]] &lt;- read_csv(snow_files[i]) %&gt;% select(Year,DOY,Sno_Height_M) } ## Rows: 6211 Columns: 52 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (52): ArrayID, Year, DOY, Hour, LoAir_Min_C, LoAir_Min_Time, LoAir_Max_C... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. ## Rows: 6575 Columns: 48 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (48): ArrayID, Year, DOY, Hour, LoAir_Min_C, LoAir_Min_Time, LoAir_Max_C... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. snow_data_full &lt;- do.call(&#39;rbind&#39;,empty_data) summary(snow_data_full) ## Year DOY Sno_Height_M ## Min. :2003 Min. : 1.0 Min. :-3.523 ## 1st Qu.:2008 1st Qu.: 92.0 1st Qu.: 0.350 ## Median :2012 Median :183.0 Median : 0.978 ## Mean :2012 Mean :183.1 Mean : 0.981 ## 3rd Qu.:2016 3rd Qu.:274.0 3rd Qu.: 1.520 ## Max. :2021 Max. :366.0 Max. : 2.905 ## NA&#39;s :4554 2.3.2 Read in the data as a map function #making the data as a map function our_snow_reader &lt;- function(file){ name = str_split_fixed(file,&#39;/&#39;,2)[,2] %&gt;% gsub(&#39;_24hr.csv&#39;,&#39;&#39;,.) df &lt;- read_csv(file) %&gt;% select(Year,DOY,Sno_Height_M) %&gt;% mutate(site = name) } #creating an object with the functions snow_data_full &lt;- map_dfr(snow_files,our_snow_reader) ## Rows: 6211 Columns: 52 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (52): ArrayID, Year, DOY, Hour, LoAir_Min_C, LoAir_Min_Time, LoAir_Max_C... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. ## Rows: 6575 Columns: 48 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (48): ArrayID, Year, DOY, Hour, LoAir_Min_C, LoAir_Min_Time, LoAir_Max_C... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. summary(snow_data_full) ## Year DOY Sno_Height_M site ## Min. :2003 Min. : 1.0 Min. :-3.523 Length:12786 ## 1st Qu.:2008 1st Qu.: 92.0 1st Qu.: 0.350 Class :character ## Median :2012 Median :183.0 Median : 0.978 Mode :character ## Mean :2012 Mean :183.1 Mean : 0.981 ## 3rd Qu.:2016 3rd Qu.:274.0 3rd Qu.: 1.520 ## Max. :2021 Max. :366.0 Max. : 2.905 ## NA&#39;s :4554 2.3.3 Plot snow data #making an object for the yearly snow data points snow_yearly &lt;- snow_data_full %&gt;% group_by(Year,site) %&gt;% summarize(mean_height = mean(Sno_Height_M,na.rm=T)) ## `summarise()` has grouped output by &#39;Year&#39;. You can override using the `.groups` argument. #plotting the yearly snow data ggplot(snow_yearly,aes(x=Year,y=mean_height,color=site)) + geom_point() + ggthemes::theme_few() + ggthemes::scale_color_few()+ labs(x=&quot;Mean Height&quot;, y=&quot;Year&quot;, title=&quot;Yearly Snow Data&quot;) ## Warning: Removed 2 rows containing missing values (geom_point). 2.4 Extracting the meteorological data URLs I used the rvest package to get the URLs for the SASP forcing and SBSP_forcing meteorological datasets. #creating values for the meteorological data URLs links_hw &lt;- webpage %&gt;% html_nodes(&#39;a&#39;) %&gt;% .[grepl(&#39;forcing&#39;,.)] %&gt;% html_attr(&#39;href&#39;) links_hw ## [1] &quot;https://snowstudies.org/wp-content/uploads/2022/02/SBB_SASP_Forcing_Data.txt&quot; ## [2] &quot;https://snowstudies.org/wp-content/uploads/2022/02/SBB_SBSP_Forcing_Data.txt&quot; 2.5 Download the meteorological data. Next, I used the download_file and str_split_fixed commands, along with a for loop, to download the data and saved it in our data folder. # grab only the name of the file by splitting out on forward slashes splits_hw &lt;- str_split_fixed(links_hw, &#39;/&#39;, 8) forcingdataset &lt;- splits_hw[,8] %&gt;% gsub(&#39;.txt&#39;,&#39;&#39;,.) file_names_hw &lt;- paste0(&#39;/Users/maddiebean21/Desktop/School/ESS580A7/bookdown_final/data/03-hw-snow/SBB_SASP_Forcing_Data&#39;, forcingdataset) # creating a for loop for(i in 1:length(file_names_hw)){ download.file(links_hw[i], destfile=file_names_hw[i]) } file_names_hw ## [1] &quot;/Users/maddiebean21/Desktop/School/ESS580A7/bookdown_final/data/03-hw-snow/SBB_SASP_Forcing_DataSBB_SASP_Forcing_Data&quot; ## [2] &quot;/Users/maddiebean21/Desktop/School/ESS580A7/bookdown_final/data/03-hw-snow/SBB_SASP_Forcing_DataSBB_SBSP_Forcing_Data&quot; 2.6 Custom function writing I wrote a custom function in order to read in the data and append a site column to the data. # this code grabs the variable names from the metadata pdf file library(pdftools) ## Using poppler version 20.12.1 q3_headers &lt;- pdf_text(&#39;https://snowstudies.org/wp-content/uploads/2022/02/Serially-Complete-Metadata-text08.pdf&#39;) %&gt;% readr::read_lines(.) %&gt;% trimws(.) %&gt;% str_split_fixed(.,&#39;\\\\.&#39;,2) %&gt;% .[,2] %&gt;% .[1:26] %&gt;% str_trim(side = &quot;left&quot;) q3_headers ## [1] &quot;year&quot; ## [2] &quot;month&quot; ## [3] &quot;day&quot; ## [4] &quot;hour&quot; ## [5] &quot;minute&quot; ## [6] &quot;second&quot; ## [7] &quot;precip [kg m-2 s-1]&quot; ## [8] &quot;sw down [W m-2]&quot; ## [9] &quot;lw down [W m-2]&quot; ## [10] &quot;air temp [K]&quot; ## [11] &quot;windspeed [m s-1]&quot; ## [12] &quot;relative humidity [%]&quot; ## [13] &quot;pressure [Pa]&quot; ## [14] &quot;specific humidity [g g-1]&quot; ## [15] &quot;calculated dewpoint temperature [K]&quot; ## [16] &quot;precip, WMO-corrected [kg m-2 s-1]&quot; ## [17] &quot;air temp, corrected with Kent et al. (1993) [K]&quot; ## [18] &quot;air temp, corrected with Anderson and Baumgartner (1998)[K]&quot; ## [19] &quot;air temp, corrected with Nakamura and Mahrt (2005) [K]&quot; ## [20] &quot;air temp, corrected with Huwald et al. (2009) [K]&quot; ## [21] &quot;qc code precip&quot; ## [22] &quot;qc code sw down&quot; ## [23] &quot;qc code lw down&quot; ## [24] &quot;qc code air temp&quot; ## [25] &quot;qc code wind speed&quot; ## [26] &quot;qc code relhum&quot; #creating a function q3_reader &lt;- function(file){ name = str_split_fixed(file, &#39;/&#39;, 2)[,2] name2 = str_split_fixed(file, &#39;_&#39;, 4)[,2] q3test = read.delim(file, header = FALSE, sep =&quot;&quot;, col.names = q3_headers, skip = 4) %&gt;% select(1:14) %&gt;% mutate(site=name2) } 2.7 Summary of meteorlogical files I used the map function to read in both meteorological files and then displayed a summary of my tibble. #reading in the forcing data forcing_data_full &lt;- map_dfr(file_names_hw, q3_reader) summary(forcing_data_full) ## year month day hour minute ## Min. :2003 Min. : 1.000 Min. : 1.00 Min. : 0.00 Min. :0 ## 1st Qu.:2005 1st Qu.: 3.000 1st Qu.: 8.00 1st Qu.: 5.75 1st Qu.:0 ## Median :2007 Median : 6.000 Median :16.00 Median :11.50 Median :0 ## Mean :2007 Mean : 6.472 Mean :15.76 Mean :11.50 Mean :0 ## 3rd Qu.:2009 3rd Qu.: 9.000 3rd Qu.:23.00 3rd Qu.:17.25 3rd Qu.:0 ## Max. :2011 Max. :12.000 Max. :31.00 Max. :23.00 Max. :0 ## second precip..kg.m.2.s.1. sw.down..W.m.2. lw.down..W.m.2. ## Min. :0 Min. :0.000e+00 Min. :-9999.000 Min. :-9999.0 ## 1st Qu.:0 1st Qu.:0.000e+00 1st Qu.: -3.510 1st Qu.: 173.4 ## Median :0 Median :0.000e+00 Median : -0.344 Median : 231.4 ## Mean :0 Mean :3.838e-05 Mean :-1351.008 Mean :-1325.7 ## 3rd Qu.:0 3rd Qu.:0.000e+00 3rd Qu.: 294.900 3rd Qu.: 272.2 ## Max. :0 Max. :6.111e-03 Max. : 1341.000 Max. : 365.8 ## air.temp..K. windspeed..m.s.1. relative.humidity.... pressure..Pa. ## Min. :242.1 Min. :-9999.000 Min. : 0.011 Min. :63931 ## 1st Qu.:265.8 1st Qu.: 0.852 1st Qu.: 37.580 1st Qu.:63931 ## Median :272.6 Median : 1.548 Median : 59.910 Median :65397 ## Mean :272.6 Mean : -790.054 Mean : 58.891 Mean :65397 ## 3rd Qu.:279.7 3rd Qu.: 3.087 3rd Qu.: 81.600 3rd Qu.:66863 ## Max. :295.8 Max. : 317.300 Max. :324.800 Max. :66863 ## specific.humidity..g.g.1. site ## Min. :0.000000 Length:138336 ## 1st Qu.:0.001744 Class :character ## Median :0.002838 Mode :character ## Mean :0.003372 ## 3rd Qu.:0.004508 ## Max. :0.014780 ##Average yearly temperature I made a line plot of mean temp by year by site (using the air temp [K] variable). What is suspicious about this data is temperature variation at the beginning of the graph, especially since the temperature is in Kelvin. This eludes that temperature was most likely not collected in the earlier years. #creating an object to plot, grabbing the mean temperature by year by site q5_yearly &lt;- forcing_data_full %&gt;% group_by(year,site) %&gt;% summarize(mean_temp_k = mean(air.temp..K.,na.rm=T)) ## `summarise()` has grouped output by &#39;year&#39;. You can override using the `.groups` argument. #plotting the mean temperature by year by site ggplot(q5_yearly) + geom_line(aes(x=year,y=mean_temp_k,color=site)) + ggthemes::theme_few() + ggthemes::scale_color_few() What is suspicious about this data is temperature variation at the beginning of the graph, especially since the temperature is in Kelvin. This eludes that temperature was most likely not collected in the earlier years. 2.8 Monthy average temperature plot Here, i wrote a function in order to make line plots of monthly average temperature at each site for a given year. I used a for loop to make these plots for 2005 to 2010. Both sites follow the same trend for each month, however, the SBSP site is never warmer than SASP site. There are times where are almost the same temperature, but SBSP never exceeds SASP. #creating a function for the monthly average temperature forcingmonthyear &lt;- function(forcing_data_full, year){ monthlytemp&lt;-forcing_data_full %&gt;% group_by(month,year,site) %&gt;% summarize(monthly_air_temp = mean(air.temp..K.,na.rm=T)) %&gt;% filter(yr == year) #plotting the function plots &lt;- ggplot(monthlytemp, aes(x = month, y = monthly_air_temp, color = site)) + geom_line(size=2)+ facet_wrap(~year) labs(title= monthlytemp$year, x = &quot;Month&quot;, y = &quot;Temperature (K)&quot;) print(plots) } years &lt;- c(2005,2006,2007,2008,2009,2010) #creating a for loop for (yr in years) { forcingmonthyear(forcing_data_full, year) } ## `summarise()` has grouped output by &#39;month&#39;, &#39;year&#39;. You can override using the `.groups` argument. ## `summarise()` has grouped output by &#39;month&#39;, &#39;year&#39;. You can override using the `.groups` argument. ## `summarise()` has grouped output by &#39;month&#39;, &#39;year&#39;. You can override using the `.groups` argument. ## `summarise()` has grouped output by &#39;month&#39;, &#39;year&#39;. You can override using the `.groups` argument. ## `summarise()` has grouped output by &#39;month&#39;, &#39;year&#39;. You can override using the `.groups` argument. ## `summarise()` has grouped output by &#39;month&#39;, &#39;year&#39;. You can override using the `.groups` argument. 2.9 Average daily precipitation For this graph, I grouped the data by day by site to get the daily temperature. # making an object for daily temperature dailytemp&lt;-forcing_data_full %&gt;% group_by(day,site) %&gt;% summarize(daily_air_temp = mean(air.temp..K.,na.rm=T)) ## `summarise()` has grouped output by &#39;day&#39;. You can override using the `.groups` argument. #plotting bonus ggplot(dailytemp, aes(x=day, y=daily_air_temp, color=site))+ geom_line()+ labs(x=&#39;Day of the Month&#39;, y=&#39;Daily Air Temperature&#39;, title = &quot;Daily Temperature&quot;) "],["lagos-analysis-part-1.html", "Chapter 3 LAGOS Analysis part 1 3.1 Loading in data 3.2 Map outline of Iowa and Illinois (similar to Minnesota map upstream) 3.3 Subsetting LAGOS data to these sites 3.4 Distribution of lake size in Iowa vs. Minnesota 3.5 Interactive plot of lakes in Iowa and Illinois, colored them by lake area in hectares 3.6 Other data sources that I might use to understand how reservoirs and natural lakes vary in size in these three states?", " Chapter 3 LAGOS Analysis part 1 3.1 Loading in data 3.1.1 First download and then specifically grab the locus (or site lat longs) # #Lagos download script LAGOSNE::lagosne_get(dest_folder = LAGOSNE:::lagos_path()) ## Warning in LAGOSNE::lagosne_get(dest_folder = LAGOSNE:::lagos_path()): LAGOSNE data for this version already exists on the local machine. ## Re-download if neccessary using the &#39;overwrite` argument.&#39; #Load in lagos lagos &lt;- lagosne_load() ## Warning in (function (version = NULL, fpath = NA) : LAGOSNE version unspecified, ## loading version: 1.087.3 #Grab the lake centroid info lake_centers &lt;- lagos$locus 3.1.2 Convert to spatial data #Look at the column names #names(lake_centers) #Look at the structure #str(lake_centers) #View the full dataset #View(lake_centers %&gt;% slice(1:100)) spatial_lakes &lt;- st_as_sf(lake_centers,coords=c(&#39;nhd_long&#39;,&#39;nhd_lat&#39;), crs=4326) %&gt;% st_transform(2163) #Subset for plotting subset_spatial &lt;- spatial_lakes %&gt;% slice(1:100) subset_baser &lt;- spatial_lakes[1:100,] #Dynamic mapviewer mapview(subset_spatial) 3.1.3 Subset to only Minnesota states &lt;- us_states() #Plot all the states to check if they loaded #mapview(states) minnesota &lt;- states %&gt;% filter(name == &#39;Minnesota&#39;) %&gt;% st_transform(2163) #Subset lakes based on spatial position minnesota_lakes &lt;- spatial_lakes[minnesota,] #Plotting the first 1000 lakes minnesota_lakes %&gt;% arrange(-lake_area_ha) %&gt;% slice(1:1000) %&gt;% mapview(.,zcol = &#39;lake_area_ha&#39;) 3.2 Map outline of Iowa and Illinois (similar to Minnesota map upstream) #creating Iowa map iowa &lt;- states %&gt;% filter(name == &#39;Iowa&#39;)%&gt;% st_transform(2163) #creating Illinois map illinois &lt;- states %&gt;% filter(name == &#39;Illinois&#39;)%&gt;% st_transform(2163) #combining Iowa and Illinois il_ia &lt;- rbind(iowa, illinois) #mapping Iowa and Illinois mapview(il_ia) mapview code chunk is from https://r-spatial.github.io/mapview/articles/articles/mapview_02-advanced.html 3.3 Subsetting LAGOS data to these sites #Subset iowa and illinois lakes based on spatial position il_ia_lakes &lt;- spatial_lakes[il_ia,] Combined, there are 16,466 lakes in Illinois and Iowa. Minnesota alone, has 29,038 lakes. This means that Minnesota has 12,572 more lakes than Illinois and Iowa. 3.4 Distribution of lake size in Iowa vs. Minnesota Here I made a histogram plot with lake size on x-axis and frequency on y axis. #making iowa lakes iowa_lakes &lt;- spatial_lakes[iowa,] #combining iowa and minnesota iamn &lt;- rbind(iowa, minnesota) #subsetting spatial lakes for Minnesota and Iowa ia_mn_lakes &lt;- spatial_lakes %&gt;% .[iamn,] %&gt;% st_join(iamn) #graphing minnesota and iowa together on ggplot ggplot()+ geom_histogram(filter(ia_mn_lakes,name == &quot;Minnesota&quot;), mapping = aes(lake_area_ha), bins = 30,color=&quot;darkblue&quot;, fill=&quot;lightblue&quot;)+ scale_x_log10() + labs(title = &quot;Minnesota Lake Size&quot;, x = &quot;Lake Area (ha)&quot;, y = &quot;Frequency&quot;) + geom_histogram(filter(ia_mn_lakes,name == &quot;Iowa&quot;), mapping = aes(lake_area_ha), bins = 30, color = &quot;purple&quot;, fill = &quot;pink&quot;) + scale_x_log10()+ labs(title = &#39;Iowa vs Minnesota Lake Size&#39;, x = &quot;Lake Area (ha)&quot;, y = &#39;Frequency&#39;)+ facet_wrap(~name, nrow = 2) ## Scale for &#39;x&#39; is already present. Adding another scale for &#39;x&#39;, which will ## replace the existing scale. https://stackoverflow.com/questions/47596357/overlaying-two-ggplot-facet-wrap-histograms helped me write some of the histogram code chunk (for visualizations) 3.5 Interactive plot of lakes in Iowa and Illinois, colored them by lake area in hectares #plotting iowa and illinois together il_ia_lakes %&gt;% arrange(-lake_area_ha) %&gt;% slice(1:1000) %&gt;% mapview(.,zcol = &#39;lake_area_ha&#39;) 3.6 Other data sources that I might use to understand how reservoirs and natural lakes vary in size in these three states? Using data sets to quantify and gain a better understanding of lakes and reservoirs are very beneficial when looking at hydrological patterns, especially since climate change is looming over our heads. A journal from Michael Meyer, et. al, discusses how they combined different global datasets to create the most harmonic dataset that contains lake surface area, and water quantity and quality, and aquifer recharge at local, regional, and global scales. They combined, Global Water Bodies Data Base, LANDSAT data, and GRACE data to create this data set. This article shows how important it is to collect different types of data in order to create and discover the workings of our hydro-world. This large data set could help us understand the reservoirs and natural lakes in these states by showing us the recharge rates of the reservoirs. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
