[["index.html", "Final Project Introduction View GitHub Code", " Final Project Madeline Bean 2022-03-30 Introduction This bookdown website is comprised of all assignments completed in Introduction to Data Science Course ESS 580A7. I completed this class in my spring semester of 2022 while getting my Professional Science Master’s in Ecosystem Sciences and Sustainability. View GitHub Code The code used to build this R Bookdown is hosted on GitHub. Final Project GitHub "],["poudre-river-interactive-graph.html", "Chapter 1 Poudre River Interactive Graph 1.1 Methods 1.2 Site Description 1.3 Data Acquisition and Plotting Tests 1.4 Interactive Graph 1.5 Information about the Poudre River", " Chapter 1 Poudre River Interactive Graph The Poudre River runs through northern Colorado, passing through Fort Collins, CO. In this assignment, we look at the the annual discharge of the river to determine annual patterns and severe weather events. 1.1 Methods The Poudre River at Lincoln Bridge is: Downstream of only a little bit of urban stormwater Near Odell Brewing CO Near an open space area and the Poudre River Trail Downstream of many agricultural diversions 1.2 Site Description 1.3 Data Acquisition and Plotting Tests 1.3.1 Data Download q &lt;- readNWISdv(siteNumbers = &#39;06752260&#39;, parameterCd = &#39;00060&#39;, startDate = &#39;2017-01-01&#39;, endDate = &#39;2022-01-01&#39;) %&gt;% rename(q = &#39;X_00060_00003&#39;) 1.3.2 Static Data Plotter ggplot(q, aes(x = Date, y = q)) + geom_line() + ylab(&#39;Q (cfs)&#39;) + ggtitle(&#39;Discharge in the Poudre River, Fort Collins&#39;) 1.3.3 Interactive Data Plotter q_xts &lt;- xts(q$q, order.by = q$Date) #plotting dygraph(q_xts) %&gt;% dyAxis(&quot;y&quot;, label = &quot;Discharge (cfs)&quot;) 1.4 Interactive Graph #creating an interactive graph dygraph(q_xts) %&gt;% dyOptions(drawPoints = TRUE, pointSize = 2) 1.5 Information about the Poudre River The History of the Poudre River The Cache la Poudre River got its name from a group of French trappers that hid their gun powder on the banks of the river to lighten their wagons. Hence the translation of the name to be the “hiding place of the powder.” Since the 1800’s, the Poudre River has been a vital resource for the Northern Colorado community. From industrial to agricultural and to residential use, the water from the Poudre river is in great demand. Poudre River Geomorphology The Poudre River starts high in the Rocky Mountain National Park peaks and flows north east through the Roosevelt National Forest, slowly making its way through the city of Fort Collins. There are a lot of recreational activities that occur along the river, including hiking, biking, kayaking, and white water rafting. However, because of the multiple reservoirs that consume water from the Poudre, the once rapid, flowing river is now only a trickling stream in some areas, especially during the winter months. "],["hayman-fire-recovery.html", "Chapter 2 Hayman Fire Recovery 2.1 Reading in the Data 2.2 NDVI and NDMI Correlation 2.3 Average NDSI and NDVI Correlation 2.4 Snow Effect Differences 2.5 Average Greenest Month 2.6 Average Snowiest Month", " Chapter 2 Hayman Fire Recovery The Hayman Fire occurred on June 8th, 2002. This was the largest wildfire in Colorado’s history until 2020, burning over 138,000 acres of land. In this assignment, we looked at how the fire affected vegetation growth in the area. library(tidyverse) library(tidyr) library(ggthemes) library(lubridate) library(ggpubr) # Now that we have learned how to munge (manipulate) data # and plot it, we will work on using these skills in new ways knitr::opts_knit$set(root.dir=&#39;..&#39;) 2.1 Reading in the Data ####-----Reading in Data and Stacking it ----- #### #Reading in files files &lt;- list.files(&#39;02-hw-hayman&#39;,full.names=T) #Read in individual data files ndmi &lt;- read_csv(&#39;/Users/maddiebean21/Desktop/School/ESS580A7/bookdown_final/data/02-hw-hayman/hayman_ndmi.csv&#39;) %&gt;% rename(burned=2,unburned=3) %&gt;% mutate(data=&#39;ndmi&#39;) ndsi &lt;- read_csv(&#39;/Users/maddiebean21/Desktop/School/ESS580A7/bookdown_final/data/02-hw-hayman/hayman_ndsi.csv&#39;) %&gt;% rename(burned=2,unburned=3) %&gt;% mutate(data=&#39;ndsi&#39;) ndvi &lt;- read_csv(&#39;/Users/maddiebean21/Desktop/School/ESS580A7/bookdown_final/data/02-hw-hayman/hayman_ndvi.csv&#39;)%&gt;% rename(burned=2,unburned=3) %&gt;% mutate(data=&#39;ndvi&#39;) # Stack as a tidy dataset full_long &lt;- rbind(ndvi,ndmi,ndsi) %&gt;% gather(key=&#39;site&#39;,value=&#39;value&#39;,-DateTime,-data) %&gt;% filter(!is.na(value)) 2.2 NDVI and NDMI Correlation In order to find the correlation between NDVI and NDMI, we converted the full_long dataset into a wide dataset using the function “spread” and then made a plot of this function, grouped by unburned vs burned sites. We excluded winter months and focused on summer months because vegetation does not generally grow in the winter. #changing data into wide full_wide &lt;- spread(data=full_long,key=&#39;data&#39;,value=&#39;value&#39;) %&gt;% filter_if(is.numeric,all_vars(!is.na(.))) %&gt;% mutate(month = month(DateTime), year = year(DateTime)) #filtering summer months summer_only &lt;- filter(full_wide,month %in% c(6,7,8,9)) #plotting summer months of variables ggplot(summer_only,aes(x=ndmi,y=ndvi,color=site)) + geom_point() + theme_few() + scale_color_few() + theme(legend.position=c(0.8,0.8)) There is a strong, positive correlation between NDMI and NDVI. 2.3 Average NDSI and NDVI Correlation In order to find the correlation between the average NDSI and NDVI, we used the data points for NDSI only during January - April (snow season) and only data points for NDVI during June - August (vegetation growth season). We found that there is a low positive correlation of 0.180 between snow coverage and vegetation. This means that the previous year’s snow cover has little effect on the vegetation growth for the following summer and that this correlation could be from other factors in the environment. Looking at the graph, you can see a difference in correlation between NDVI and NDSI burned and un-burned sites. #variable ndsi months var.snow_months &lt;- c(1,2,3,4) #variable ndvi months var.growth_months &lt;- c(6,7,8) #mean NDSI per year ndsi_avg &lt;- full_wide[c(&quot;DateTime&quot;, &quot;ndsi&quot;, &quot;month&quot;, &quot;year&quot;, &quot;site&quot;)] %&gt;% filter(month %in% var.snow_months) %&gt;% group_by(site, year) %&gt;% summarize(ndsi_avg=mean(ndsi)) #mean NDVI per year ndvi_avg &lt;- full_wide[c(&quot;DateTime&quot;, &quot;ndvi&quot;, &quot;month&quot;, &quot;year&quot;, &quot;site&quot;)] %&gt;% filter(month %in% var.growth_months) %&gt;% group_by(site, year) %&gt;% summarize(ndvi_avg=mean(ndvi)) #combining NDVI and NDSI into one dataset combined &lt;- inner_join(ndvi_avg, ndsi_avg) #correlation cor(combined$ndvi_avg, combined$ndsi_avg) ## [1] 0.1803564 ggplot(combined, aes(x=ndsi_avg, y=ndvi_avg, color=site)) + geom_point() + theme_few() + scale_color_few() + theme(legend.position=c(0.8,0.8)) 2.4 Snow Effect Differences We then looked at the difference of snow effect from the previous correlation between pre- and post-burn and burned and unburned. #snow effect pre-burn preburn &lt;- c(1984:2001) #snow effect post-burn postburn &lt;- c(2003:2019) #snow effect average for preburn ndsi_preburn_avg &lt;- full_wide[c(&quot;DateTime&quot;, &quot;ndsi&quot;, &quot;month&quot;, &quot;year&quot;, &quot;site&quot;)] %&gt;% filter(month %in% var.snow_months) %&gt;% filter(year %in% preburn) %&gt;% group_by(site, year) %&gt;% summarize(ndsi_preburn_avg=mean(ndsi)) #vegetation effect average for preburn ndvi_preburn_avg &lt;- full_wide[c(&quot;DateTime&quot;, &quot;ndvi&quot;, &quot;month&quot;, &quot;year&quot;, &quot;site&quot;)] %&gt;% filter(month %in% var.growth_months) %&gt;% filter(year %in% preburn) %&gt;% group_by(site, year) %&gt;% summarize(ndvi_preburn_avg=mean(ndvi)) #combing preburn ndsi and ndvi into one data set combine_preburn &lt;- inner_join(ndsi_preburn_avg, ndvi_preburn_avg) #correlation for preburn cor(combine_preburn$ndsi_preburn_avg, combine_preburn$ndvi_preburn_avg) ## [1] 0.09100615 #snow effect average for postburn ndsi_postburn_avg &lt;- full_wide[c(&quot;DateTime&quot;, &quot;ndsi&quot;, &quot;month&quot;, &quot;year&quot;, &quot;site&quot;)] %&gt;% filter(month %in% var.snow_months) %&gt;% filter(year %in% postburn) %&gt;% group_by(site, year) %&gt;% summarize(ndsi_postburn_avg=mean(ndsi)) #vegetation effect average for postburn ndvi_postburn_avg &lt;- full_wide[c(&quot;DateTime&quot;, &quot;ndvi&quot;, &quot;month&quot;, &quot;year&quot;, &quot;site&quot;)] %&gt;% filter(month %in% var.growth_months) %&gt;% filter(year %in% postburn) %&gt;% group_by(site, year) %&gt;% summarize(ndvi_postburn_avg=mean(ndvi)) #combing postburn ndsi and ndvi into one data set combine_postburn &lt;- inner_join(ndsi_postburn_avg, ndvi_postburn_avg) #correlation for postburn cor(combine_postburn$ndsi_postburn_avg, combine_postburn$ndvi_postburn_avg) ## [1] 0.24394 #snow effect average for burned ndsi_burned_avg &lt;- full_wide[c(&quot;DateTime&quot;, &quot;ndsi&quot;, &quot;month&quot;, &quot;year&quot;, &quot;site&quot;)] %&gt;% filter(month %in% var.snow_months) %&gt;% filter(site %in% &quot;burned&quot;) %&gt;% group_by(site, year) %&gt;% summarize(ndsi_burned_avg=mean(ndsi)) #snow effect average for unburned ndsi_unburned_avg &lt;- full_wide[c(&quot;DateTime&quot;, &quot;ndsi&quot;, &quot;month&quot;, &quot;year&quot;, &quot;site&quot;)] %&gt;% filter(month %in% var.snow_months) %&gt;% filter(site %in% &quot;unburned&quot;) %&gt;% group_by(site, year) %&gt;% summarize(ndsi_unburned_avg=mean(ndsi)) #vegetation effect average for burned ndvi_burned_avg &lt;- full_wide[c(&quot;DateTime&quot;, &quot;ndvi&quot;, &quot;month&quot;, &quot;year&quot;, &quot;site&quot;)] %&gt;% filter(month %in% var.growth_months) %&gt;% filter(site %in% &quot;burned&quot;) %&gt;% group_by(site, year) %&gt;% summarize(ndvi_burned_avg=mean(ndvi)) #vegetation effect average for unburned ndvi_unburned_avg &lt;- full_wide[c(&quot;DateTime&quot;, &quot;ndvi&quot;, &quot;month&quot;, &quot;year&quot;, &quot;site&quot;)] %&gt;% filter(month %in% var.growth_months) %&gt;% filter(site %in% &quot;unburned&quot;) %&gt;% group_by(site, year) %&gt;% summarize(ndvi_unburned_avg=mean(ndvi)) #combining data for burned combined_burned &lt;- inner_join(ndvi_burned_avg, ndsi_burned_avg) #combining data for unburned combined_unburned &lt;- inner_join(ndvi_unburned_avg, ndsi_unburned_avg) #correlation for burned data cor(combined_burned$ndvi_burned_avg, combined_burned$ndsi_burned_avg) ## [1] 0.08700527 #correlation for unburned data cor(combined_unburned$ndvi_unburned_avg, combined_unburned$ndsi_unburned_avg) ## [1] -0.03100231 #graphing the data Preburngraph &lt;- ggplot(combine_preburn, aes(x=ndsi_preburn_avg, y=ndvi_preburn_avg)) + geom_point() + theme_few() + scale_color_few() + labs(x=&quot;Pre-burn NDSI Average&quot;, y=&quot;Pre-burn NDVI Average&quot;) Postburngraph &lt;- ggplot(combine_postburn, aes(x=ndsi_postburn_avg, y=ndvi_postburn_avg)) + geom_point() + theme_few() + scale_color_few()+ labs(x=&quot;Post-burn NDSI Average&quot;, y = &quot;Post-burn NDVI Average&quot;) Burnedgraph &lt;- ggplot(combined_burned, aes(x=ndsi_burned_avg, y=ndvi_burned_avg)) + geom_point() + theme_few() + scale_color_few()+ labs(x=&quot;NDSI Burned Average&quot;, y=&quot;NDVI Burned Average&quot;) Unburnedgraph &lt;- ggplot(combined_unburned, aes(x=ndsi_unburned_avg, y=ndvi_unburned_avg))+ geom_point() + theme_few() + scale_color_few()+ labs(x= &quot;Average NDSI Unburned&quot;, y=&quot;Average NDVI Unburned&quot;) #Plotting the data in one frame Plot &lt;- ggarrange(Preburngraph, Postburngraph, Burnedgraph, Unburnedgraph, labels = c(&quot;Pre-burned&quot;, &quot;Post-burned&quot;, &quot;Burned&quot;, &quot;Unburned&quot;), ncol = 2, nrow = 2) Plot For pre-burn snow effect, the correlation is 0.091. For post-burn snow effect, the correlation is 0.244. For un-burned snow effect, the correlation is -0.031 For burned snow effect, the correlation is 0.087. The snow effect from question 2 is a more generalized correlation. While all of these variables have a weak correlation, this question isolates the different variables in the data to help determine the relationship between snow coverage and vegetation growth. The post-burn condition has the strongest correlation between the previous year’s snow coverage and the vegetation growth for the following year. The weakest correlation between snow coverage and vegetation growth was for the un-burned condition. This is interesting because this could potentially mean that the Hayman Fire has pushed the vegetation growth to rely more on snow coverage to get their nutrients. This is still a weak correlation, so more research will need to be conducted to determine if this is true or not; there are many other variables in the environment that could have caused this correlation. 2.5 Average Greenest Month Here, we looked at which month was the greenest, on average. We discovered that August was the greenest month. #creating an object for the average vegetation for each month ndvigreenest &lt;- full_wide[c(&quot;DateTime&quot;, &quot;ndvi&quot;, &quot;month&quot;, &quot;year&quot;, &quot;site&quot;)] %&gt;% group_by(month) %&gt;% summarize(ndvi=mean(ndvi)) #Plotting the average vegetation for each month ggplot(ndvigreenest, aes(x=month, y=ndvi)) + geom_bar(stat=&quot;identity&quot;, fill=&quot;#009E73&quot;) + theme_minimal() 2.6 Average Snowiest Month Next, we looked at which month was the snowiest on average. January was the snowiest month. #creating snow object ndsisnowiest &lt;- full_wide[c(&quot;DateTime&quot;, &quot;ndsi&quot;, &quot;month&quot;, &quot;year&quot;, &quot;site&quot;)] %&gt;% group_by(month) %&gt;% summarize(ndsi=mean(ndsi)) #plotting the snow for each month ggplot(ndsisnowiest, aes(x=month, y=ndsi)) + geom_bar(stat=&quot;identity&quot;, fill=&quot;#56B4E9&quot;) + theme_minimal() "],["snow-data-example.html", "Chapter 3 Snow Data Example 3.1 Reading an html 3.2 Data Download 3.3 Data read-in 3.4 Extracting the meteorological data URLs 3.5 Download the meteorological data. 3.6 Custom function writing 3.7 Summary of meteorlogical files 3.8 Monthy average temperature plot 3.9 Average daily precipitation", " Chapter 3 Snow Data Example In this assignment, I explored web scraping, different functions and iterations by using a data set from the Center for Snow and Avalanche Studies Website and read a table in. This table contains links to data I want and to programatically download for three sites. I don’t know much about these sites, but they contain incredibly rich snow, temperature, and precipitation data. 3.1 Reading an html 3.1.1 Extract CSV links from webpage site_url &lt;- &#39;https://snowstudies.org/archived-data/&#39; #Read the web url webpage &lt;- read_html(site_url) #See if we can extract tables and get the data that way tables &lt;- webpage %&gt;% html_nodes(&#39;table&#39;) %&gt;% magrittr::extract2(3) %&gt;% html_table(fill = TRUE) #That didn&#39;t work, so let&#39;s try a different approach #Extract only weblinks and then the URLs! links &lt;- webpage %&gt;% html_nodes(&#39;a&#39;) %&gt;% .[grepl(&#39;24hr&#39;,.)] %&gt;% html_attr(&#39;href&#39;) 3.2 Data Download 3.2.1 Download data in a for loop #Grab only the name of the file by splitting out on forward slashes splits &lt;- str_split_fixed(links,&#39;/&#39;,8) #Keep only the 8th column dataset &lt;- splits[,8] #generate a file list for where the data goes file_names &lt;- paste0(&#39;/Users/maddiebean21/Desktop/School/ESS580A7/bookdown_final/data/03-hw-snow/SASP_24hr.csv&#39;,dataset) for(i in 1:3){ download.file(links[i],destfile=file_names[i]) } downloaded &lt;- file.exists(file_names) evaluate &lt;- !all(downloaded) 3.2.2 Download data in a map #Map version of the same for loop (downloading 3 files) if(evaluate == T){ map2(links[1:3],file_names[1:3],download.file) }else{print(&#39;data already downloaded&#39;)} ## [[1]] ## [1] 0 ## ## [[2]] ## [1] 0 ## ## [[3]] ## [1] 0 3.3 Data read-in 3.3.1 Read in just the snow data as a loop #Pattern matching to only keep certain files snow_files &lt;- file_names %&gt;% .[!grepl(&#39;SG_24&#39;,.)] %&gt;% .[!grepl(&#39;PTSP&#39;,.)] empty_data &lt;- list() snow_data &lt;- for(i in 1:length(snow_files)){ empty_data[[i]] &lt;- read_csv(snow_files[i]) %&gt;% select(Year,DOY,Sno_Height_M) } ## Rows: 6211 Columns: 52 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (52): ArrayID, Year, DOY, Hour, LoAir_Min_C, LoAir_Min_Time, LoAir_Max_C... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. ## Rows: 6575 Columns: 48 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (48): ArrayID, Year, DOY, Hour, LoAir_Min_C, LoAir_Min_Time, LoAir_Max_C... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. snow_data_full &lt;- do.call(&#39;rbind&#39;,empty_data) summary(snow_data_full) ## Year DOY Sno_Height_M ## Min. :2003 Min. : 1.0 Min. :-3.523 ## 1st Qu.:2008 1st Qu.: 92.0 1st Qu.: 0.350 ## Median :2012 Median :183.0 Median : 0.978 ## Mean :2012 Mean :183.1 Mean : 0.981 ## 3rd Qu.:2016 3rd Qu.:274.0 3rd Qu.: 1.520 ## Max. :2021 Max. :366.0 Max. : 2.905 ## NA&#39;s :4554 3.3.2 Read in the data as a map function #making the data as a map function our_snow_reader &lt;- function(file){ name = str_split_fixed(file,&#39;/&#39;,2)[,2] %&gt;% gsub(&#39;_24hr.csv&#39;,&#39;&#39;,.) df &lt;- read_csv(file) %&gt;% select(Year,DOY,Sno_Height_M) %&gt;% mutate(site = name) } #creating an object with the functions snow_data_full &lt;- map_dfr(snow_files,our_snow_reader) ## Rows: 6211 Columns: 52 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (52): ArrayID, Year, DOY, Hour, LoAir_Min_C, LoAir_Min_Time, LoAir_Max_C... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. ## Rows: 6575 Columns: 48 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (48): ArrayID, Year, DOY, Hour, LoAir_Min_C, LoAir_Min_Time, LoAir_Max_C... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. summary(snow_data_full) ## Year DOY Sno_Height_M site ## Min. :2003 Min. : 1.0 Min. :-3.523 Length:12786 ## 1st Qu.:2008 1st Qu.: 92.0 1st Qu.: 0.350 Class :character ## Median :2012 Median :183.0 Median : 0.978 Mode :character ## Mean :2012 Mean :183.1 Mean : 0.981 ## 3rd Qu.:2016 3rd Qu.:274.0 3rd Qu.: 1.520 ## Max. :2021 Max. :366.0 Max. : 2.905 ## NA&#39;s :4554 3.3.3 Plot snow data #making an object for the yearly snow data points snow_yearly &lt;- snow_data_full %&gt;% group_by(Year,site) %&gt;% summarize(mean_height = mean(Sno_Height_M,na.rm=T)) ## `summarise()` has grouped output by &#39;Year&#39;. You can override using the `.groups` argument. #plotting the yearly snow data ggplot(snow_yearly,aes(x=Year,y=mean_height,color=site)) + geom_point() + ggthemes::theme_few() + ggthemes::scale_color_few()+ labs(x=&quot;Mean Height&quot;, y=&quot;Year&quot;, title=&quot;Yearly Snow Data&quot;) ## Warning: Removed 2 rows containing missing values (geom_point). 3.4 Extracting the meteorological data URLs I used the rvest package to get the URLs for the SASP forcing and SBSP_forcing meteorological datasets. #creating values for the meteorological data URLs links_hw &lt;- webpage %&gt;% html_nodes(&#39;a&#39;) %&gt;% .[grepl(&#39;forcing&#39;,.)] %&gt;% html_attr(&#39;href&#39;) links_hw ## [1] &quot;https://snowstudies.org/wp-content/uploads/2022/02/SBB_SASP_Forcing_Data.txt&quot; ## [2] &quot;https://snowstudies.org/wp-content/uploads/2022/02/SBB_SBSP_Forcing_Data.txt&quot; 3.5 Download the meteorological data. Next, I used the download_file and str_split_fixed commands, along with a for loop, to download the data and saved it in our data folder. # grab only the name of the file by splitting out on forward slashes splits_hw &lt;- str_split_fixed(links_hw, &#39;/&#39;, 8) forcingdataset &lt;- splits_hw[,8] %&gt;% gsub(&#39;.txt&#39;,&#39;&#39;,.) file_names_hw &lt;- paste0(&#39;/Users/maddiebean21/Desktop/School/ESS580A7/bookdown_final/data/03-hw-snow/SBB_SASP_Forcing_Data&#39;, forcingdataset) # creating a for loop for(i in 1:length(file_names_hw)){ download.file(links_hw[i], destfile=file_names_hw[i]) } file_names_hw ## [1] &quot;/Users/maddiebean21/Desktop/School/ESS580A7/bookdown_final/data/03-hw-snow/SBB_SASP_Forcing_DataSBB_SASP_Forcing_Data&quot; ## [2] &quot;/Users/maddiebean21/Desktop/School/ESS580A7/bookdown_final/data/03-hw-snow/SBB_SASP_Forcing_DataSBB_SBSP_Forcing_Data&quot; 3.6 Custom function writing I wrote a custom function in order to read in the data and append a site column to the data. # this code grabs the variable names from the metadata pdf file library(pdftools) ## Using poppler version 20.12.1 q3_headers &lt;- pdf_text(&#39;https://snowstudies.org/wp-content/uploads/2022/02/Serially-Complete-Metadata-text08.pdf&#39;) %&gt;% readr::read_lines(.) %&gt;% trimws(.) %&gt;% str_split_fixed(.,&#39;\\\\.&#39;,2) %&gt;% .[,2] %&gt;% .[1:26] %&gt;% str_trim(side = &quot;left&quot;) q3_headers ## [1] &quot;year&quot; ## [2] &quot;month&quot; ## [3] &quot;day&quot; ## [4] &quot;hour&quot; ## [5] &quot;minute&quot; ## [6] &quot;second&quot; ## [7] &quot;precip [kg m-2 s-1]&quot; ## [8] &quot;sw down [W m-2]&quot; ## [9] &quot;lw down [W m-2]&quot; ## [10] &quot;air temp [K]&quot; ## [11] &quot;windspeed [m s-1]&quot; ## [12] &quot;relative humidity [%]&quot; ## [13] &quot;pressure [Pa]&quot; ## [14] &quot;specific humidity [g g-1]&quot; ## [15] &quot;calculated dewpoint temperature [K]&quot; ## [16] &quot;precip, WMO-corrected [kg m-2 s-1]&quot; ## [17] &quot;air temp, corrected with Kent et al. (1993) [K]&quot; ## [18] &quot;air temp, corrected with Anderson and Baumgartner (1998)[K]&quot; ## [19] &quot;air temp, corrected with Nakamura and Mahrt (2005) [K]&quot; ## [20] &quot;air temp, corrected with Huwald et al. (2009) [K]&quot; ## [21] &quot;qc code precip&quot; ## [22] &quot;qc code sw down&quot; ## [23] &quot;qc code lw down&quot; ## [24] &quot;qc code air temp&quot; ## [25] &quot;qc code wind speed&quot; ## [26] &quot;qc code relhum&quot; #creating a function q3_reader &lt;- function(file){ name = str_split_fixed(file, &#39;/&#39;, 2)[,2] name2 = str_split_fixed(file, &#39;_&#39;, 4)[,2] q3test = read.delim(file, header = FALSE, sep =&quot;&quot;, col.names = q3_headers, skip = 4) %&gt;% select(1:14) %&gt;% mutate(site=name2) } 3.7 Summary of meteorlogical files I used the map function to read in both meteorological files and then displayed a summary of my tibble. #reading in the forcing data forcing_data_full &lt;- map_dfr(file_names_hw, q3_reader) summary(forcing_data_full) ## year month day hour minute ## Min. :2003 Min. : 1.000 Min. : 1.00 Min. : 0.00 Min. :0 ## 1st Qu.:2005 1st Qu.: 3.000 1st Qu.: 8.00 1st Qu.: 5.75 1st Qu.:0 ## Median :2007 Median : 6.000 Median :16.00 Median :11.50 Median :0 ## Mean :2007 Mean : 6.472 Mean :15.76 Mean :11.50 Mean :0 ## 3rd Qu.:2009 3rd Qu.: 9.000 3rd Qu.:23.00 3rd Qu.:17.25 3rd Qu.:0 ## Max. :2011 Max. :12.000 Max. :31.00 Max. :23.00 Max. :0 ## second precip..kg.m.2.s.1. sw.down..W.m.2. lw.down..W.m.2. ## Min. :0 Min. :0.000e+00 Min. :-9999.000 Min. :-9999.0 ## 1st Qu.:0 1st Qu.:0.000e+00 1st Qu.: -3.510 1st Qu.: 173.4 ## Median :0 Median :0.000e+00 Median : -0.344 Median : 231.4 ## Mean :0 Mean :3.838e-05 Mean :-1351.008 Mean :-1325.7 ## 3rd Qu.:0 3rd Qu.:0.000e+00 3rd Qu.: 294.900 3rd Qu.: 272.2 ## Max. :0 Max. :6.111e-03 Max. : 1341.000 Max. : 365.8 ## air.temp..K. windspeed..m.s.1. relative.humidity.... pressure..Pa. ## Min. :242.1 Min. :-9999.000 Min. : 0.011 Min. :63931 ## 1st Qu.:265.8 1st Qu.: 0.852 1st Qu.: 37.580 1st Qu.:63931 ## Median :272.6 Median : 1.548 Median : 59.910 Median :65397 ## Mean :272.6 Mean : -790.054 Mean : 58.891 Mean :65397 ## 3rd Qu.:279.7 3rd Qu.: 3.087 3rd Qu.: 81.600 3rd Qu.:66863 ## Max. :295.8 Max. : 317.300 Max. :324.800 Max. :66863 ## specific.humidity..g.g.1. site ## Min. :0.000000 Length:138336 ## 1st Qu.:0.001744 Class :character ## Median :0.002838 Mode :character ## Mean :0.003372 ## 3rd Qu.:0.004508 ## Max. :0.014780 ##Average yearly temperature I made a line plot of mean temp by year by site (using the air temp [K] variable). What is suspicious about this data is temperature variation at the beginning of the graph, especially since the temperature is in Kelvin. This eludes that temperature was most likely not collected in the earlier years. #creating an object to plot, grabbing the mean temperature by year by site q5_yearly &lt;- forcing_data_full %&gt;% group_by(year,site) %&gt;% summarize(mean_temp_k = mean(air.temp..K.,na.rm=T)) ## `summarise()` has grouped output by &#39;year&#39;. You can override using the `.groups` argument. #plotting the mean temperature by year by site ggplot(q5_yearly) + geom_line(aes(x=year,y=mean_temp_k,color=site)) + ggthemes::theme_few() + ggthemes::scale_color_few() What is suspicious about this data is temperature variation at the beginning of the graph, especially since the temperature is in Kelvin. This eludes that temperature was most likely not collected in the earlier years. 3.8 Monthy average temperature plot Here, i wrote a function in order to make line plots of monthly average temperature at each site for a given year. I used a for loop to make these plots for 2005 to 2010. Both sites follow the same trend for each month, however, the SBSP site is never warmer than SASP site. There are times where are almost the same temperature, but SBSP never exceeds SASP. #creating a function for the monthly average temperature forcingmonthyear &lt;- function(forcing_data_full, year){ monthlytemp&lt;-forcing_data_full %&gt;% group_by(month,year,site) %&gt;% summarize(monthly_air_temp = mean(air.temp..K.,na.rm=T)) %&gt;% filter(yr == year) #plotting the function plots &lt;- ggplot(monthlytemp, aes(x = month, y = monthly_air_temp, color = site)) + geom_line(size=2)+ facet_wrap(~year) labs(title= monthlytemp$year, x = &quot;Month&quot;, y = &quot;Temperature (K)&quot;) print(plots) } years &lt;- c(2005,2006,2007,2008,2009,2010) #creating a for loop for (yr in years) { forcingmonthyear(forcing_data_full, year) } ## `summarise()` has grouped output by &#39;month&#39;, &#39;year&#39;. You can override using the `.groups` argument. ## `summarise()` has grouped output by &#39;month&#39;, &#39;year&#39;. You can override using the `.groups` argument. ## `summarise()` has grouped output by &#39;month&#39;, &#39;year&#39;. You can override using the `.groups` argument. ## `summarise()` has grouped output by &#39;month&#39;, &#39;year&#39;. You can override using the `.groups` argument. ## `summarise()` has grouped output by &#39;month&#39;, &#39;year&#39;. You can override using the `.groups` argument. ## `summarise()` has grouped output by &#39;month&#39;, &#39;year&#39;. You can override using the `.groups` argument. 3.9 Average daily precipitation For this graph, I grouped the data by day by site to get the daily temperature. # making an object for daily temperature dailytemp&lt;-forcing_data_full %&gt;% group_by(day,site) %&gt;% summarize(daily_air_temp = mean(air.temp..K.,na.rm=T)) ## `summarise()` has grouped output by &#39;day&#39;. You can override using the `.groups` argument. #plotting bonus ggplot(dailytemp, aes(x=day, y=daily_air_temp, color=site))+ geom_line()+ labs(x=&#39;Day of the Month&#39;, y=&#39;Daily Air Temperature&#39;, title = &quot;Daily Temperature&quot;) "],["lagos-analysis-part-1.html", "Chapter 4 LAGOS Analysis part 1 4.1 Loading in data 4.2 Map outline of Iowa and Illinois (similar to Minnesota map upstream) 4.3 Subsetting LAGOS data to these sites 4.4 Distribution of lake size in Iowa vs. Minnesota 4.5 Interactive plot of lakes in Iowa and Illinois, colored them by lake area in hectares 4.6 Other data sources that I might use to understand how reservoirs and natural lakes vary in size in these three states?", " Chapter 4 LAGOS Analysis part 1 In this assignment, I used the R package ‘sf’ and other R packages to explore spatial operations in R markdown. I used data from LAGOS database to look at clean lake data in Minnesota, Illinois, and Iowa. 4.1 Loading in data 4.1.1 First download and then specifically grab the locus (or site lat longs) # #Lagos download script LAGOSNE::lagosne_get(dest_folder = LAGOSNE:::lagos_path()) ## Warning in LAGOSNE::lagosne_get(dest_folder = LAGOSNE:::lagos_path()): LAGOSNE data for this version already exists on the local machine. ## Re-download if neccessary using the &#39;overwrite` argument.&#39; #Load in lagos lagos &lt;- lagosne_load() ## Warning in (function (version = NULL, fpath = NA) : LAGOSNE version unspecified, ## loading version: 1.087.3 #Grab the lake centroid info lake_centers &lt;- lagos$locus 4.1.2 Convert to spatial data #Look at the column names #names(lake_centers) #Look at the structure #str(lake_centers) #View the full dataset #View(lake_centers %&gt;% slice(1:100)) spatial_lakes &lt;- st_as_sf(lake_centers,coords=c(&#39;nhd_long&#39;,&#39;nhd_lat&#39;), crs=4326) %&gt;% st_transform(2163) #Subset for plotting subset_spatial &lt;- spatial_lakes %&gt;% slice(1:100) subset_baser &lt;- spatial_lakes[1:100,] #Dynamic mapviewer mapview(subset_spatial) 4.1.3 Subset to only Minnesota states &lt;- us_states() #Plot all the states to check if they loaded #mapview(states) minnesota &lt;- states %&gt;% filter(name == &#39;Minnesota&#39;) %&gt;% st_transform(2163) #Subset lakes based on spatial position minnesota_lakes &lt;- spatial_lakes[minnesota,] #Plotting the first 1000 lakes minnesota_lakes %&gt;% arrange(-lake_area_ha) %&gt;% slice(1:1000) %&gt;% mapview(.,zcol = &#39;lake_area_ha&#39;) 4.2 Map outline of Iowa and Illinois (similar to Minnesota map upstream) #creating Iowa map iowa &lt;- states %&gt;% filter(name == &#39;Iowa&#39;)%&gt;% st_transform(2163) #creating Illinois map illinois &lt;- states %&gt;% filter(name == &#39;Illinois&#39;)%&gt;% st_transform(2163) #combining Iowa and Illinois il_ia &lt;- rbind(iowa, illinois) #mapping Iowa and Illinois mapview(il_ia) mapview code chunk is from https://r-spatial.github.io/mapview/articles/articles/mapview_02-advanced.html 4.3 Subsetting LAGOS data to these sites #Subset iowa and illinois lakes based on spatial position il_ia_lakes &lt;- spatial_lakes[il_ia,] Combined, there are 16,466 lakes in Illinois and Iowa. Minnesota alone, has 29,038 lakes. This means that Minnesota has 12,572 more lakes than Illinois and Iowa. 4.4 Distribution of lake size in Iowa vs. Minnesota Here I made a histogram plot with lake size on x-axis and frequency on y axis. #making iowa lakes iowa_lakes &lt;- spatial_lakes[iowa,] #combining iowa and minnesota iamn &lt;- rbind(iowa, minnesota) #subsetting spatial lakes for Minnesota and Iowa ia_mn_lakes &lt;- spatial_lakes %&gt;% .[iamn,] %&gt;% st_join(iamn) #graphing minnesota and iowa together on ggplot ggplot()+ geom_histogram(filter(ia_mn_lakes,name == &quot;Minnesota&quot;), mapping = aes(lake_area_ha), bins = 30,color=&quot;darkblue&quot;, fill=&quot;lightblue&quot;)+ scale_x_log10() + labs(title = &quot;Minnesota Lake Size&quot;, x = &quot;Lake Area (ha)&quot;, y = &quot;Frequency&quot;) + geom_histogram(filter(ia_mn_lakes,name == &quot;Iowa&quot;), mapping = aes(lake_area_ha), bins = 30, color = &quot;purple&quot;, fill = &quot;pink&quot;) + scale_x_log10()+ labs(title = &#39;Iowa vs Minnesota Lake Size&#39;, x = &quot;Lake Area (ha)&quot;, y = &#39;Frequency&#39;)+ facet_wrap(~name, nrow = 2) ## Scale for &#39;x&#39; is already present. Adding another scale for &#39;x&#39;, which will ## replace the existing scale. https://stackoverflow.com/questions/47596357/overlaying-two-ggplot-facet-wrap-histograms helped me write some of the histogram code chunk (for visualizations) 4.5 Interactive plot of lakes in Iowa and Illinois, colored them by lake area in hectares #plotting iowa and illinois together il_ia_lakes %&gt;% arrange(-lake_area_ha) %&gt;% slice(1:1000) %&gt;% mapview(.,zcol = &#39;lake_area_ha&#39;) 4.6 Other data sources that I might use to understand how reservoirs and natural lakes vary in size in these three states? Using data sets to quantify and gain a better understanding of lakes and reservoirs are very beneficial when looking at hydrological patterns, especially since climate change is looming over our heads. A journal from Michael Meyer, et. al, discusses how they combined different global datasets to create the most harmonic dataset that contains lake surface area, and water quantity and quality, and aquifer recharge at local, regional, and global scales. They combined, Global Water Bodies Data Base, LANDSAT data, and GRACE data to create this data set. This article shows how important it is to collect different types of data in order to create and discover the workings of our hydro-world. This large data set could help us understand the reservoirs and natural lakes in these states by showing us the recharge rates of the reservoirs. "],["lagos-analysis-part-2.html", "Chapter 5 LAGOS Analysis Part 2 5.1 Loading in data 5.2 The correlation between Secchi Disk Depth and Chlorophyll a for sites with at least 200 observations 5.3 Why might this be the case? 5.4 States that have the most data 5.5 Spatial pattern in Secchi disk depth for lakes with at least 200 observations", " Chapter 5 LAGOS Analysis Part 2 In this second part of the LAGOS Analysis assignment, I also explored spatial operations in R. In this assignment however, I looked at the water quality in lakes and how secchi disk depth is correlated to the amount of chlorophyll in the lake. I also looked at the amount of data for each state. 5.1 Loading in data 5.1.1 First download and then specifically grab the locus (or site lat longs) #Lagos download script #lagosne_get(dest_folder = LAGOSNE:::lagos_path(),overwrite=T) #Load in lagos lagos &lt;- lagosne_load() #Grab the lake centroid info lake_centers &lt;- lagos$locus # Make an sf object spatial_lakes &lt;- st_as_sf(lake_centers,coords=c(&#39;nhd_long&#39;,&#39;nhd_lat&#39;), crs=4326) #Grab the water quality data nutr &lt;- lagos$epi_nutr #Look at column names #names(nutr) 5.1.2 Subset columns nutr to only keep key info that I wanted clarity_only &lt;- nutr %&gt;% select(lagoslakeid,sampledate,chla,doc,secchi) %&gt;% mutate(sampledate = as.character(sampledate) %&gt;% ymd(.)) 5.1.3 Keep sites with at least 200 observations #Look at the number of rows of dataset #nrow(clarity_only) chla_secchi &lt;- clarity_only %&gt;% filter(!is.na(chla), !is.na(secchi)) # How many observatiosn did we lose? # nrow(clarity_only) - nrow(chla_secchi) # Keep only the lakes with at least 200 observations of secchi and chla chla_secchi_200 &lt;- chla_secchi %&gt;% group_by(lagoslakeid) %&gt;% mutate(count = n()) %&gt;% filter(count &gt; 200) 5.1.4 Join water quality data to spatial data spatial_200 &lt;- inner_join(spatial_lakes,chla_secchi_200 %&gt;% distinct(lagoslakeid,.keep_all=T), by=&#39;lagoslakeid&#39;) 5.1.5 Mean Chl_a map ### Take the mean chl_a and secchi by lake mean_values_200 &lt;- chla_secchi_200 %&gt;% # Take summary by lake id group_by(lagoslakeid) %&gt;% # take mean chl_a per lake id summarize(mean_chl = mean(chla,na.rm=T), mean_secchi=mean(secchi,na.rm=T)) %&gt;% #Get rid of NAs filter(!is.na(mean_chl), !is.na(mean_secchi)) %&gt;% # Take the log base 10 of the mean_chl mutate(log10_mean_chl = log10(mean_chl)) #Join datasets mean_spatial &lt;- inner_join(spatial_lakes,mean_values_200, by=&#39;lagoslakeid&#39;) #Make a map mapview(mean_spatial,zcol=&#39;log10_mean_chl&#39;) 5.2 The correlation between Secchi Disk Depth and Chlorophyll a for sites with at least 200 observations Here, I plotted chla vs secchi for all sites #plotting chla vs secchi for sites with at least 200 observations ggplot(chla_secchi_200, aes(x=chla, y = secchi)) + geom_point(color = &quot;darkblue&quot;)+ scale_x_log10()+ scale_y_log10() ## Warning: Transformation introduced infinite values in continuous x-axis cor(chla_secchi_200$chla, chla_secchi_200$secchi) ## [1] -0.3774089 5.3 Why might this be the case? The correlation of the Secchi Disk Depth and Chlorophyll A is -0.3774. This low negative correlation could be because the more Chlorophyll A in the water causes the water to become more turbid, which causes the Secchi Disk Depth to decrease. 5.4 States that have the most data 5.4.1 First I made a lagos spatial dataset that has the total number of counts per site. code for question 2 was from Amanda Hastings on the code help - had a lot of trouble and got frustrated, which led me to cave and use Amanda’s code #creating counts object with the lagos spatial data set site_counts &lt;- lake_centers %&gt;% group_by(lagoslakeid) %&gt;% mutate(count = n()) %&gt;% select(lagoslakeid, nhd_long, nhd_lat, count, state_zoneid) 5.4.2 Second, I joined this point dataset to the us_boundaries data. #joining the states with the lagos counts states_lagos &lt;- lagos$state %&gt;% select(-state_pct_in_nwi, -state_ha_in_nwi,-state_ha) stateid_counts &lt;- inner_join(site_counts, states_lagos, by=&quot;state_zoneid&quot;) 5.4.3 Then I grouped by state and sum all the observations in that state and arrange that data from most to least total observations per state. #grouping by state and summarizing counts in each state state_sums &lt;- stateid_counts %&gt;% group_by(state_name) %&gt;% summarize(sum_counts = sum(count)) states &lt;- us_states() #joining states and the counts together states_join &lt;- inner_join(states,state_sums,by=&#39;state_name&#39;) %&gt;% arrange(desc(sum_counts)) #mapping the sum counts based on state mapview(states_join, zcol=&#39;sum_counts&#39;) The state that has the largest amount of data is Minnesota with 29,022. The state with the 2nd most amount of data is Michigan with 15,569. 5.5 Spatial pattern in Secchi disk depth for lakes with at least 200 observations #mapping the secchi disk depth for lakes with at least 200 using the spatial 200 table spatial_200 %&gt;% arrange(-secchi)%&gt;% mapview(.,zcol = &#39;secchi&#39;) There does not seem to be an obvious pattern, however, the secchi disk depths data are clustered in areas of Minnesota, Wisconsin, Missouri, and the Northeast. This could be because of agricultural land being prevalent in these areas. The effects of farming can have an effect on the quality of nearby lakes, which is potentially why there are clusters of secchi disk depth for lakes with at least 200 observations (because they want to monitor these effects). "],["corn-analysis.html", "Chapter 6 Corn Analysis 6.1 Weather Data Analysis 6.2 Temperature trends 6.3 Extract Winneshiek County corn yields, fit a linear time trend, make a plot. 6.4 Fitting a quadratic time trend (i.e., year + year^2) and making a plot. 6.5 Time Series: Analyzing the relationship between temperature and yields for the Winneshiek County time series. 6.6 Cross-Section: Analyzing the relationship between temperature and yield across all counties in 2018. 6.7 Panel: Plot comparing actual and fitted yields and interpret the results of your model. 6.8 Soybeans: Time series plot", " Chapter 6 Corn Analysis In this last assignment, I used temperature from the PRISM database and corn yield data from the NASS database to create linear regression models. We looked at summer and winter temperature trends to explore the relationship of summer temperatures and corn yield. 6.1 Weather Data Analysis 6.1.1 Load the PRISM daily maximum temperatures #daily max temperature #dimensions: counties by days by years prism &lt;- readMat(&quot;/Users/maddiebean21/Desktop/School/ESS580A7/bookdown_final/data/05-hw-corn/prismiowa.mat&quot;) # look at county 1 t_1981_c1 &lt;- prism$tmaxdaily.iowa[1,,1] t_1981_c1[366] ## [1] NaN plot(1:366, t_1981_c1, type = &quot;l&quot;) #plotting ggplot() + geom_line(mapping = aes(x=1:366, y = t_1981_c1)) + theme_bw() + xlab(&quot;day of year&quot;) + ylab(&quot;daily maximum temperature (°C)&quot;) + ggtitle(&quot;Daily Maximum Temperature, Iowa County #1&quot;) ## Warning: Removed 1 row(s) containing missing values (geom_path). # assign dimension names to tmax matrix dimnames(prism$tmaxdaily.iowa) &lt;- list(prism$COUNTYFP, 1:366, prism$years) # converted 3d matrix into a data frame tmaxdf &lt;- as.data.frame.table(prism$tmaxdaily.iowa) # relabel the columns colnames(tmaxdf) &lt;- c(&quot;countyfp&quot;,&quot;doy&quot;,&quot;year&quot;,&quot;tmax&quot;) tmaxdf &lt;- tibble(tmaxdf) 6.2 Temperature trends 6.2.1 Summer temperature trends: Winneshiek County # making the data numeric tmaxdf$doy &lt;- as.numeric(tmaxdf$doy) tmaxdf$year &lt;- as.numeric(as.character(tmaxdf$year)) winnesummer &lt;- tmaxdf %&gt;% filter(countyfp==191 &amp; doy &gt;= 152 &amp; doy &lt;= 243) %&gt;% group_by(year) %&gt;% summarize(meantmax = mean(tmax)) ggplot(winnesummer, mapping = aes(x = year, y = meantmax)) + geom_point() + theme_bw() + labs(x = &quot;year&quot;, y = &quot;Tmax (°C)&quot;) + geom_smooth(method = lm) ## `geom_smooth()` using formula &#39;y ~ x&#39; lm_summertmax &lt;- lm(meantmax ~ year, winnesummer) summary(lm_summertmax) ## ## Call: ## lm(formula = meantmax ~ year, data = winnesummer) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.5189 -0.7867 -0.0341 0.6859 3.7415 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 41.57670 36.44848 1.141 0.262 ## year -0.00747 0.01823 -0.410 0.684 ## ## Residual standard error: 1.232 on 36 degrees of freedom ## Multiple R-squared: 0.004644, Adjusted R-squared: -0.02301 ## F-statistic: 0.168 on 1 and 36 DF, p-value: 0.6844 6.2.2 Winter Temperatures - Winneshiek County winnewinter &lt;- tmaxdf %&gt;% filter(countyfp==191 &amp; (doy &lt;= 59 | doy &gt;= 335) &amp; !is.na(tmax)) %&gt;% group_by(year) %&gt;% summarize(meantmax = mean(tmax)) ggplot(winnewinter, mapping = aes(x = year, y = meantmax)) + geom_point() + theme_bw() + labs(x = &quot;year&quot;, y = &quot;Tmax (°C)&quot;) + geom_smooth(method = lm) ## `geom_smooth()` using formula &#39;y ~ x&#39; lm_wintertmax &lt;- lm(meantmax ~ year, winnewinter) summary(lm_wintertmax) ## ## Call: ## lm(formula = meantmax ~ year, data = winnewinter) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.5978 -1.4917 -0.3053 1.3778 4.5709 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -29.87825 60.48100 -0.494 0.624 ## year 0.01368 0.03025 0.452 0.654 ## ## Residual standard error: 2.045 on 36 degrees of freedom ## Multiple R-squared: 0.005652, Adjusted R-squared: -0.02197 ## F-statistic: 0.2046 on 1 and 36 DF, p-value: 0.6537 6.2.3 Multiple regression – Quadratic time trend winnewinter$yearsq &lt;- winnewinter$year^2 lm_wintertmaxquad &lt;- lm(meantmax ~ year + yearsq, winnewinter) summary(lm_wintertmaxquad) ## ## Call: ## lm(formula = meantmax ~ year + yearsq, data = winnewinter) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.3539 -1.2985 -0.2813 1.4055 4.2620 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.086e+04 1.238e+04 -0.877 0.386 ## year 1.085e+01 1.239e+01 0.876 0.387 ## yearsq -2.710e-03 3.097e-03 -0.875 0.388 ## ## Residual standard error: 2.051 on 35 degrees of freedom ## Multiple R-squared: 0.02694, Adjusted R-squared: -0.02867 ## F-statistic: 0.4845 on 2 and 35 DF, p-value: 0.6201 winnewinter$fitted &lt;- lm_wintertmaxquad$fitted.values ggplot(winnewinter) + geom_point(mapping = aes(x = year, y = meantmax)) + geom_line(mapping = aes(x = year, y = fitted)) + theme_bw() + labs(x = &quot;year&quot;, y = &quot;tmax&quot;) 6.2.4 Download NASS corn yield data # set our API key with NASS nassqs_auth(key = &quot;693A4922-893D-3AF4-A23B-6288B220EC7E&quot;) # parameters to query on params &lt;- list(commodity_desc = &quot;CORN&quot;, util_practice_desc = &quot;GRAIN&quot;, prodn_practice_desc = &quot;ALL PRODUCTION PRACTICES&quot;, year__GE = 1981, state_alpha = &quot;IA&quot;) # download cornyieldsall &lt;- nassqs_yields(params) cornyieldsall$county_ansi &lt;- as.numeric(cornyieldsall$county_ansi) cornyieldsall$yield &lt;- as.numeric(cornyieldsall$Value) # clean and filter this dataset cornyields &lt;- select(cornyieldsall, county_ansi, county_name, yield, year) %&gt;% filter(!is.na(county_ansi) &amp; !is.na(yield)) cornyields &lt;- tibble(cornyields) 6.3 Extract Winneshiek County corn yields, fit a linear time trend, make a plot. #creating an object for Winneshiek County corn yields winnecornyield &lt;- cornyields %&gt;% filter(county_ansi==&quot;191&quot;) #fitting a linear time trend lm_winnecornyield &lt;- lm(yield ~ year, data = winnecornyield) summary(lm_winnecornyield) ## ## Call: ## lm(formula = yield ~ year, data = winnecornyield) ## ## Residuals: ## Min 1Q Median 3Q Max ## -51.163 -1.841 2.363 9.437 24.376 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -4763.290 448.286 -10.63 4.46e-13 *** ## year 2.457 0.224 10.96 1.77e-13 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 16.97 on 39 degrees of freedom ## Multiple R-squared: 0.7551, Adjusted R-squared: 0.7488 ## F-statistic: 120.2 on 1 and 39 DF, p-value: 1.767e-13 #plotting Winneshiek&#39;s corn yields with linear trend ggplot(winnecornyield, mapping = aes(x = year, y = yield)) + geom_point() + theme_bw() + labs(x = &quot;year&quot;, y = &quot;yield&quot;, title = &quot;Linear Time Trend Winneshiek County&quot;) + geom_smooth(method = lm) ## `geom_smooth()` using formula &#39;y ~ x&#39; The plot shows that the data following relatively close to the trend line, giving us a significant time trend. The p-value given from the linear model is significantly below 0.05, which tells us that there is a strong relationship between year and yield in Winneshiek County. 6.4 Fitting a quadratic time trend (i.e., year + year^2) and making a plot. #adding years^2 column to the winneshiek corn yield data set winnecornyield$yearsq &lt;- winnecornyield$year^2 #creating a linear model for the quadratic winneshiek corn yield data set lm_winnecornyieldquad &lt;- lm(yield ~ year + yearsq, winnecornyield) summary(lm_winnecornyieldquad) ## ## Call: ## lm(formula = yield ~ year + yearsq, data = winnecornyield) ## ## Residuals: ## Min 1Q Median 3Q Max ## -51.384 -3.115 1.388 9.743 25.324 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.583e+04 8.580e+04 0.301 0.765 ## year -2.812e+01 8.576e+01 -0.328 0.745 ## yearsq 7.641e-03 2.143e-02 0.357 0.723 ## ## Residual standard error: 17.17 on 38 degrees of freedom ## Multiple R-squared: 0.7559, Adjusted R-squared: 0.7431 ## F-statistic: 58.84 on 2 and 38 DF, p-value: 2.311e-12 #adding fitted values column to the winneshiek corn yield data set winnecornyield$fitted &lt;- lm_winnecornyieldquad$fitted.values #Plotting the quadratic time trend ggplot(winnecornyield) + geom_point(mapping = aes(x = year, y = yield)) + geom_line(mapping = aes(x = year, y = fitted)) + theme_bw() + labs(x = &quot;year&quot;, y = &quot;yield&quot;, title = &quot;Quadratic Time Trend Winneshiek County&quot;) The trend line/slope is still increasing, not showing us an evidence of slowing down yield. 6.5 Time Series: Analyzing the relationship between temperature and yields for the Winneshiek County time series. #combining summer and corn yield data sets winnecombinedsummer &lt;- inner_join(winnecornyield, winnesummer) ## Joining, by = &quot;year&quot; #adding column for squared temperature winnecombinedsummer$tempsq &lt;- winnecombinedsummer$meantmax^2 #linear model lm_comb_summer_quad &lt;- lm(yield ~ meantmax + tempsq, winnecombinedsummer) summary(lm_comb_summer_quad) ## ## Call: ## lm(formula = yield ~ meantmax + tempsq, data = winnecombinedsummer) ## ## Residuals: ## Min 1Q Median 3Q Max ## -56.587 -22.262 -0.982 22.409 52.798 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -4223.604 1446.639 -2.920 0.00609 ** ## meantmax 328.918 107.068 3.072 0.00410 ** ## tempsq -6.173 1.979 -3.119 0.00362 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 29.5 on 35 degrees of freedom ## Multiple R-squared: 0.2417, Adjusted R-squared: 0.1984 ## F-statistic: 5.579 on 2 and 35 DF, p-value: 0.007887 winnecombinedsummer$quadfitted &lt;- lm_comb_summer_quad$fitted.values #plotting the data together ggplot(winnecombinedsummer) + geom_point(mapping = aes(x = meantmax, y = yield)) + geom_line(mapping = aes(x=meantmax, y=quadfitted))+ theme_bw() + labs(x = &quot;temperature&quot;, y = &quot;yield&quot;, title = &quot;Temperature vs. Yield Winneshiek County&quot;) For this plot, I used data on yield and summer average maximum temperatures. Adding tmax^2 to my model is helpful because it helps us visualize that more average temperatures produce that highest yield, where the extreme low or high temp do not produce high yields. Plotting years on this graph would not be beneficial because it would only show the yield increasing as the years pass. The p-value is 0.007887, which is less than alpha, meaning that there is a significant evidence that when temperature is between 26 - 28 degrees Celsius, yields are the highest. 6.6 Cross-Section: Analyzing the relationship between temperature and yield across all counties in 2018. #creating object for corn yields for 2018 yields18 &lt;- cornyields %&gt;% filter(year == 2018) %&gt;% group_by(county_name) %&gt;% unique() %&gt;% filter(!is.na(county_ansi)) #creating object for temperature for 2018 temp18 &lt;- tmaxdf %&gt;% group_by(countyfp) %&gt;% filter(year == 2018) %&gt;% filter(doy &gt;= 152 &amp; doy &lt;= 243) %&gt;% summarize(meantmax = mean(tmax)) %&gt;% rename(county_ansi = &quot;countyfp&quot;) #making counties numeric temp18$county_ansi &lt;- as.numeric(as.character(temp18$county_ansi)) #pulling temperature and yield across all counties in 2018 combo18 &lt;- inner_join(yields18, temp18, by = &#39;county_ansi&#39;) #adding column for squared mean temp combo18$meantmaxsq &lt;- combo18$meantmax^2 #linear model lm_combo18 &lt;- lm(yield~meantmax + meantmaxsq, data = combo18) summary(lm_combo18) ## ## Call: ## lm(formula = yield ~ meantmax + meantmaxsq, data = combo18) ## ## Residuals: ## Min 1Q Median 3Q Max ## -44.221 -15.399 5.007 14.541 30.879 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -5501.602 1860.830 -2.957 0.00397 ** ## meantmax 406.789 131.493 3.094 0.00263 ** ## meantmaxsq -7.256 2.321 -3.126 0.00239 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 18.75 on 90 degrees of freedom ## Multiple R-squared: 0.1317, Adjusted R-squared: 0.1124 ## F-statistic: 6.827 on 2 and 90 DF, p-value: 0.001736 #adding columns for fitted values and squared temp combo18$quadfitted &lt;- lm_combo18$fitted.values #plotting the data ggplot(combo18)+ geom_point(mapping = aes(x = meantmax, y = yield))+ geom_line(mapping = aes(x = meantmax, y = quadfitted))+ theme_bw() + labs(x = &quot;yield&quot;, y = &quot;Tmax (°C)&quot;, title = &quot;Temperature vs. Yield Across All Counties in 2018&quot;) The p-value that we get from running the linear model, is 0.06308, which is larger than alpha. This means that there is not enough evidence to support that there is a correlation between temperature and yield across all counties in 2018. 6.7 Panel: Plot comparing actual and fitted yields and interpret the results of your model. #creating a summer temperature across counties summertemp &lt;- tmaxdf %&gt;% filter(doy &gt;= 152 &amp; doy &lt;= 243) %&gt;% group_by(year, countyfp) %&gt;% rename(county_ansi = countyfp)%&gt;% summarize(meantmax = mean(tmax)) ## `summarise()` has grouped output by &#39;year&#39;. You can override using the `.groups` argument. #changing the counties to a factor cornyields$county_ansi &lt;- as.factor(cornyields$county_ansi) summertemp$county_ansi &lt;- as.factor(summertemp$county_ansi) #joining yield and temperature together countysummer &lt;- inner_join(summertemp, cornyields) %&gt;% unique() ## Joining, by = c(&quot;year&quot;, &quot;county_ansi&quot;) head(countysummer) ## # A tibble: 6 × 5 ## # Groups: year [1] ## year county_ansi meantmax county_name yield ## &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1981 93 27.5 IDA 109. ## 2 1981 147 27.1 PALO ALTO 137. ## 3 1981 13 26.6 BLACK HAWK 132. ## 4 1981 99 27.8 JASPER 124. ## 5 1981 195 26.5 WORTH 119. ## 6 1981 67 26.9 FLOYD 129. #adding squared mean to columns countysummer$meantmaxsq &lt;- countysummer$meantmax^2 #creating a linear model lm_countysummer &lt;- lm(yield ~ meantmax + meantmaxsq +county_ansi + year, countysummer) summary(lm_countysummer) ## ## Call: ## lm(formula = yield ~ meantmax + meantmaxsq + county_ansi + year, ## data = countysummer) ## ## Residuals: ## Min 1Q Median 3Q Max ## -81.645 -9.720 1.924 13.232 40.409 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -5.815e+03 9.802e+01 -59.319 &lt; 2e-16 *** ## meantmax 1.182e+02 6.108e+00 19.352 &lt; 2e-16 *** ## meantmaxsq -2.225e+00 1.085e-01 -20.503 &lt; 2e-16 *** ## county_ansi147 -4.601e+00 4.322e+00 -1.065 0.287100 ## county_ansi13 -4.573e+00 4.322e+00 -1.058 0.290041 ## county_ansi99 2.537e+00 4.322e+00 0.587 0.557264 ## county_ansi195 -5.740e+00 4.339e+00 -1.323 0.186000 ## county_ansi67 -7.072e+00 4.324e+00 -1.636 0.102029 ## county_ansi107 -9.660e+00 4.325e+00 -2.233 0.025596 * ## county_ansi15 3.120e+00 4.322e+00 0.722 0.470379 ## county_ansi57 -1.366e+00 4.327e+00 -0.316 0.752212 ## county_ansi127 2.318e+00 4.321e+00 0.536 0.591719 ## county_ansi123 -3.711e+00 4.325e+00 -0.858 0.390951 ## county_ansi81 -3.157e+00 4.327e+00 -0.729 0.465744 ## county_ansi129 -1.637e+00 4.396e+00 -0.372 0.709703 ## county_ansi1 -1.186e+01 4.325e+00 -2.743 0.006124 ** ## county_ansi175 -2.341e+01 4.354e+00 -5.377 8.03e-08 *** ## county_ansi19 -4.212e+00 4.324e+00 -0.974 0.330092 ## county_ansi37 -9.693e+00 4.328e+00 -2.240 0.025182 * ## county_ansi109 3.589e-01 4.324e+00 0.083 0.933863 ## county_ansi165 -4.189e+00 4.322e+00 -0.969 0.332408 ## county_ansi51 -2.644e+01 4.360e+00 -6.063 1.47e-09 *** ## county_ansi69 -5.518e-01 4.322e+00 -0.128 0.898414 ## county_ansi137 -5.977e+00 4.330e+00 -1.381 0.167514 ## county_ansi9 -6.795e+00 4.322e+00 -1.572 0.115958 ## county_ansi87 -6.583e+00 4.328e+00 -1.521 0.128366 ## county_ansi5 -9.147e+00 4.329e+00 -2.113 0.034697 * ## county_ansi189 -3.627e+00 4.330e+00 -0.838 0.402306 ## county_ansi53 -2.790e+01 4.356e+00 -6.404 1.70e-10 *** ## county_ansi121 -1.445e+01 4.324e+00 -3.342 0.000841 *** ## county_ansi191 -7.295e+00 4.335e+00 -1.683 0.092484 . ## county_ansi71 1.715e+00 4.344e+00 0.395 0.692933 ## county_ansi135 -2.771e+01 4.354e+00 -6.365 2.20e-10 *** ## county_ansi23 -2.774e+00 4.321e+00 -0.642 0.520945 ## county_ansi179 -1.508e+01 4.358e+00 -3.461 0.000544 *** ## county_ansi11 -4.677e+00 4.321e+00 -1.082 0.279184 ## county_ansi193 -9.064e+00 4.323e+00 -2.097 0.036096 * ## county_ansi117 -3.354e+01 4.385e+00 -7.650 2.56e-14 *** ## county_ansi139 -3.580e+00 4.324e+00 -0.828 0.407827 ## county_ansi141 2.369e+00 4.321e+00 0.548 0.583576 ## county_ansi3 -1.639e+01 4.325e+00 -3.790 0.000153 *** ## county_ansi29 -5.718e+00 4.325e+00 -1.322 0.186209 ## county_ansi7 -3.014e+01 4.355e+00 -6.922 5.22e-12 *** ## county_ansi95 -4.648e+00 4.322e+00 -1.075 0.282230 ## county_ansi77 -8.484e+00 4.324e+00 -1.962 0.049814 * ## county_ansi55 -2.440e+00 4.326e+00 -0.564 0.572813 ## county_ansi91 -1.981e+00 4.324e+00 -0.458 0.646825 ## county_ansi79 1.290e+00 4.321e+00 0.299 0.765255 ## county_ansi59 -8.957e+00 4.325e+00 -2.071 0.038446 * ## county_ansi27 -2.196e+00 4.321e+00 -0.508 0.611316 ## county_ansi187 2.350e+00 4.321e+00 0.544 0.586618 ## county_ansi41 -5.251e+00 4.322e+00 -1.215 0.224409 ## county_ansi45 -1.312e+00 4.321e+00 -0.304 0.761484 ## county_ansi153 2.166e+00 4.325e+00 0.501 0.616434 ## county_ansi65 -4.543e+00 4.329e+00 -1.050 0.293954 ## county_ansi131 -4.578e+00 4.336e+00 -1.056 0.291223 ## county_ansi143 -3.119e+00 4.326e+00 -0.721 0.470936 ## county_ansi197 -3.072e-01 4.322e+00 -0.071 0.943331 ## county_ansi133 -1.106e+01 4.328e+00 -2.556 0.010618 * ## county_ansi83 2.088e+00 4.321e+00 0.483 0.629024 ## county_ansi35 2.034e+00 4.321e+00 0.471 0.637848 ## county_ansi101 -1.133e+01 4.335e+00 -2.613 0.009012 ** ## county_ansi21 -3.222e+00 4.321e+00 -0.746 0.455896 ## county_ansi105 -4.132e+00 4.322e+00 -0.956 0.339059 ## county_ansi63 -4.630e+00 4.328e+00 -1.070 0.284740 ## county_ansi125 -9.943e+00 4.325e+00 -2.299 0.021566 * ## county_ansi157 -1.315e+00 4.323e+00 -0.304 0.761057 ## county_ansi155 -5.942e-01 4.356e+00 -0.136 0.891498 ## county_ansi169 -6.458e-01 4.321e+00 -0.149 0.881201 ## county_ansi145 -1.223e+01 4.330e+00 -2.824 0.004764 ** ## county_ansi167 3.716e+00 4.321e+00 0.860 0.389950 ## county_ansi159 -3.257e+01 4.325e+00 -7.529 6.38e-14 *** ## county_ansi89 -1.092e+01 4.346e+00 -2.513 0.012022 * ## county_ansi115 -4.533e+00 4.329e+00 -1.047 0.295100 ## county_ansi173 -2.591e+01 4.357e+00 -5.947 2.99e-09 *** ## county_ansi49 -1.050e+00 4.324e+00 -0.243 0.808084 ## county_ansi151 -3.603e-01 4.321e+00 -0.083 0.933548 ## county_ansi39 -3.591e+01 4.354e+00 -8.246 2.26e-16 *** ## county_ansi17 -5.353e-01 4.323e+00 -0.124 0.901456 ## county_ansi97 -1.325e+01 4.322e+00 -3.065 0.002190 ** ## county_ansi85 -4.971e+00 4.327e+00 -1.149 0.250644 ## county_ansi149 -4.510e+00 4.322e+00 -1.044 0.296759 ## county_ansi185 -3.376e+01 4.353e+00 -7.754 1.15e-14 *** ## county_ansi47 -5.335e+00 4.321e+00 -1.235 0.217050 ## county_ansi111 -1.008e+01 4.335e+00 -2.326 0.020060 * ## county_ansi177 -1.714e+01 4.343e+00 -3.947 8.08e-05 *** ## county_ansi75 -3.522e-01 4.321e+00 -0.081 0.935055 ## county_ansi181 -1.402e+01 4.325e+00 -3.242 0.001198 ** ## county_ansi31 3.927e+00 4.321e+00 0.909 0.363477 ## county_ansi25 -1.476e+00 4.321e+00 -0.342 0.732672 ## county_ansi61 -2.068e+00 4.328e+00 -0.478 0.632814 ## county_ansi113 -5.448e+00 4.321e+00 -1.261 0.207502 ## county_ansi183 -1.446e+00 4.328e+00 -0.334 0.738278 ## county_ansi33 -7.281e+00 4.326e+00 -1.683 0.092484 . ## county_ansi43 -2.998e+00 4.326e+00 -0.693 0.488271 ## county_ansi73 2.754e+00 4.323e+00 0.637 0.524176 ## county_ansi161 -2.473e+00 4.321e+00 -0.572 0.567164 ## county_ansi103 -7.483e+00 4.322e+00 -1.731 0.083476 . ## county_ansi119 -2.534e+00 4.321e+00 -0.586 0.557604 ## county_ansi163 4.415e+00 4.321e+00 1.022 0.306959 ## county_ansi171 -2.122e+00 4.321e+00 -0.491 0.623334 ## year 2.203e+00 2.836e-02 77.664 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 18.83 on 3646 degrees of freedom ## Multiple R-squared: 0.7207, Adjusted R-squared: 0.7129 ## F-statistic: 93.13 on 101 and 3646 DF, p-value: &lt; 2.2e-16 countysummer$fittedyield &lt;- lm_countysummer$fitted.values #plotting the data ggplot(countysummer) + geom_point(mapping = aes(x = fittedyield , y = yield))+ theme_bw()+ geom_abline(color = &quot;blue&quot;, size = 1)+ labs(title = &quot;Panel Regression Plot for Yields vs. Fitted&quot;, x=&quot;Fitted Yield Values&quot;, y = &quot;Yield Values&quot;) One way to leverage multiple time series is to group all data into what is called a “panel” regression. I converted the county ID code (“countyfp” or “county_ansi”) into factor using as.factor, then included this variable in a regression using all counties’ yield and summer temperature data. The linear model gives us an R squared value of 0.7207. This is a relatively high R squared value, which means that the data has a stronger goodness of fit. Therefore, the fitted values are well predicted in comparison to the actual yields. Because the data closely follows a 45 degree angle we can come to this conclusion. 6.8 Soybeans: Time series plot # set our API key with NASS nassqs_auth(key = &quot;693A4922-893D-3AF4-A23B-6288B220EC7E&quot;) # parameters to query on params2 &lt;- list(commodity_desc = &quot;SOYBEANS&quot;, statisticcat_desc=&quot;YIELD&quot;, prodn_practice_desc = &quot;ALL PRODUCTION PRACTICES&quot;, year__GE = 1981, state_alpha = &quot;IA&quot;) soybeanyieldsall &lt;- nassqs_yields(params2) soybeanyieldsall$county_ansi &lt;- as.numeric(soybeanyieldsall$county_ansi) soybeanyieldsall$yield &lt;- as.numeric(soybeanyieldsall$Value) # clean and filter this dataset soyyields &lt;- select(soybeanyieldsall, county_ansi, county_name, yield, year) %&gt;% filter(!is.na(county_ansi) &amp; !is.na(yield)) soyyields &lt;- tibble(soyyields) winnesoyyield &lt;- soyyields %&gt;% filter(county_ansi==&quot;191&quot;) #fitting a linear time trend lm_winnesoyyield &lt;- lm(yield ~ year, data = winnesoyyield) summary(lm_winnesoyyield) #plotting Winneshiek&#39;s soybean yields with linear trend ggplot(winnesoyyield, mapping = aes(x = year, y = yield)) + geom_point() + theme_bw() + labs(x = &quot;year&quot;, y = &quot;yield&quot;, title = ) + geom_smooth(method = lm) I downloaded NASS data on soybean yields and explored a time series relationship for Winneshiek county. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
